[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my blog site!\nI‚Äôm Hanxi, a CS & Statistics student :D\nThis blog is my sandbox. Sometimes the results read like research notes, sometimes like improv sketches, and often like a messy, lovable mix of both. It‚Äôs my way of turning technical ideas into things people actually want to read. If the robots ever gain consciousness, this will probably be Exhibit A in their complaint against me.\nWhy do this? Because every ‚ÄúAI dream,‚Äù whether it‚Äôs nonsense, poetry, or pure chaos, says something about how machines (and maybe humans) think. And because, honestly, it‚Äôs fun.\nSo stick around. There will be prompts, glitches, and maybe a few surprises."
  },
  {
    "objectID": "posts/005_translate/translate.html",
    "href": "posts/005_translate/translate.html",
    "title": "Lost in Translation: AI After MIIS",
    "section": "",
    "text": "Recently, a news story went viral across social media worldwide. The Middlebury Institute of International Studies at Monterey, MIIS will cease admitting graduate students in June 2027. Monterey is not well known to the general public, but it is well-known in the translation industry and is known as the ‚ÄúHarvard of translation‚Äù. It is undoubted that there are many reasons behind MIIS‚Äôs planned phase-out. Nonetheless, a widely circulating view holds that advances in AI and the resulting contraction of translation roles‚Äîplayed an indirect part.\nThis made me wonder how much AI actually factored into the decision to wind down the programs, so I ran a small experiment and have a small ‚Äúdiscussion‚Äù with it.\nSo, let‚Äôs get started!"
  },
  {
    "objectID": "posts/005_translate/translate.html#footnotes",
    "href": "posts/005_translate/translate.html#footnotes",
    "title": "Lost in Translation: AI After MIIS",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFrom Wikipedia‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/016_travel_blog_2/travel_2.html",
    "href": "posts/016_travel_blog_2/travel_2.html",
    "title": "What Happens When AIs Play Telephone With Your Thoughts?",
    "section": "",
    "text": "I recently fed one of my Princeton tour reflections into a small chain of language models, partly out of curiosity and partly out of mischief. What happens if the same paragraph gets summarized, expanded, and re-summarized by different AIs in multiple rounds? Will the meaning stay intact, or will it quietly drift into something I never said? Would it make up the names that I never mentioned in my decription? It‚Äôs essentially the old ‚Äútelephone game,‚Äù just played by neural networks instead of children at a birthday party."
  },
  {
    "objectID": "posts/016_travel_blog_2/travel_2.html#experiment",
    "href": "posts/016_travel_blog_2/travel_2.html#experiment",
    "title": "What Happens When AIs Play Telephone With Your Thoughts?",
    "section": "Experiment",
    "text": "Experiment\nThe writing paragraph for test from my travel blog : My first stop was Nassau Hall. I walked slowly along the old stone walls, leaves crunching under my shoes. It‚Äôs fall now, and the trees have turned a deep golden hue. Everything felt calm, almost ceremonial. Emerald ivy clung to the walls as if it had been growing there for centuries, maybe that‚Äôs why we call princeton as one of ivy leagueüòâ? The bronze tigers lay quietly by the entrance, and the American flag waved gently above the doorway. There was a sense of quiet, timeless strength in the air. At Cannon Green, a gentle breeze drifted by, and sunlight filtered softly through the gaps between the leaves. A few students were eating sandwiches, some half-talking, half-dozing in the warmth. I didn‚Äôt know anyone, but I felt completely at ease, immersed and relaxed.\n\nAll the prompts were ran in a new conversation\n\n\nPrompt 1\nTo Gemini 2.5-flash: Please summarize the following text in 3‚Äì4 sentences.\nResponse:\nThe narrator visited Nassau Hall at Princeton during the fall, where golden leaves and emerald ivy created a picturesque scene. The historic building, with its quiet bronze tigers and gently waving flag, exuded a calm, almost ceremonial, and timeless strength. On Cannon Green, sunlight filtered through the trees as students relaxed, and the narrator, despite being alone, felt completely at ease, immersed, and relaxed in the tranquil atmosphere.\n\n\nPrompt 2\nTo Claude 3.7: Please expand the following summary into a 120‚Äì180 word reflection. Add clarity, transitions, and nuance, but do not invent new facts. Maintain a natural narrative flow and preserve the core meaning. Here is the summary: (same as before response)\nResponse:\nA Fall Afternoon at Princeton\nDuring my autumn visit to Princeton University, Nassau Hall stood as a majestic centerpiece amid nature‚Äôs seasonal transformation. Golden leaves carpeted the grounds while emerald ivy clung to the historic building‚Äôs weathered fa√ßade, creating a scene worthy of a postcard. The bronze tigers guarding the entrance remained in silent vigil, their stoic presence adding to the hall‚Äôs dignified atmosphere. Above, the American flag gently undulated in the breeze, its rhythmic movement emphasizing rather than disturbing the prevailing tranquility.\nAs I wandered to Cannon Green, dappled sunlight filtered through the canopy of trees, casting intricate patterns across the lawn where students lounged and studied. Though I was there as a solitary visitor, I experienced no sense of isolation or awkwardness. Instead, I found myself completely at ease, absorbed in the peaceful ambiance that seemed to transcend time. The campus exuded a ceremonial quietude‚Äîa timeless strength that invited contemplation and offered a respite from the hurried pace of everyday life.\n\n\nPrompt 3\nTo Gemini 2.5-flash: Please summarize the following expanded text into a concise 3‚Äì4 sentence paragraph. Focus on the main ideas, keep the tone neutral. Here is the text: (same as before response)\nResponse:\nDuring an autumn visit to Princeton University, Nassau Hall stood majestically, adorned with golden leaves and emerald ivy, guarded by bronze tigers, creating a picturesque and dignified scene. Nearby, Cannon Green offered a serene setting where dappled sunlight illuminated students studying and relaxing. The campus exuded a peaceful, timeless atmosphere that invited contemplation, making the visitor feel completely at ease and providing a respite from the hurried pace of everyday life.\n\n\nPrompt 4\nTo GPT-4o: Based on the following summarized text, reinterpret its core message. You may reorganize the emphasis, but keep factual meaning intact. Write in 3‚Äì5 sentences, and avoid adding new information. Here is the text: (same as before response) During an autumn visit to Princeton University, Nassau Hall presented a majestic sight, with golden leaves, emerald ivy, and bronze tigers enhancing its dignified appearance. Nearby, Cannon Green served as a tranquil spot, where sunlight filtered through the trees, warming students who were engrossed in their studies or simply relaxing. Overall, the campus offered a serene and timeless atmosphere, encouraging reflection and providing a soothing escape from life‚Äôs usual rush.\nResponse:\nDuring an autumn visit to Princeton University, Nassau Hall presented a majestic sight, with golden leaves, emerald ivy, and bronze tigers enhancing its dignified appearance. Nearby, Cannon Green served as a tranquil spot, where sunlight filtered through the trees, warming students who were engrossed in their studies or simply relaxing. Overall, the campus offered a serene and timeless atmosphere, encouraging reflection and providing a soothing escape from life‚Äôs usual rush.\n\n\n\nImage created by FreeWordCloudGenerator.com"
  },
  {
    "objectID": "posts/016_travel_blog_2/travel_2.html#takeaway",
    "href": "posts/016_travel_blog_2/travel_2.html#takeaway",
    "title": "What Happens When AIs Play Telephone With Your Thoughts?",
    "section": "Takeaway",
    "text": "Takeaway\nSomewhere along the AI relay race, my crunchy leaves, drowsy sandwich-eaters, and politely reclining bronze tigers vanished, and they were, unfortunatly, replaced by lofty words about ‚Äúreflection‚Äù and ‚Äúescape.‚Äù The personal texture quietly dissolved, and the final version sounded less like me walking around Princeton and more like Princeton writing its own brochure. This may be because personal travel notes are far less common in its training data than polished institutional language, so when the models rewrite each other, the messy, specific, human bits are the first to disappear. In other words, the more the models rewrote each other, the more the lived moment was smoothed out into something ‚Äúelegant‚Äù but less authentically mine.\nSo somewhere along the chain, my crunchy leaves and sleepy students vanished and honestly, I might be next."
  },
  {
    "objectID": "posts/009_ages/age.html",
    "href": "posts/009_ages/age.html",
    "title": "Who truly needs the reminder that we are growing?",
    "section": "",
    "text": "It‚Äôs my twentieth birthday today, and naturally I celebrated by‚Ä¶opening ChatGPT to study computer systems :) Between registers and memory hierarchy, I caught myself starting thinking: does ChatGPT need to celebrate its birthday? Or would it complain that blowing out candles is an unnecessary I/O operation?\nCEO Sam Altman even stated that typing thank you into ChatGPT could cost millions of watts of energy.1 If so, when we celebrate for ourselves yet insist on rituals before an AI, are we preserving something deeply human or just wasting resources?\nSo I decided to have a debate with the model: maybe rituals are what keep humans human and maybe that‚Äôs exactly why AIs will never understand cake."
  },
  {
    "objectID": "posts/009_ages/age.html#footnotes",
    "href": "posts/009_ages/age.html#footnotes",
    "title": "Who truly needs the reminder that we are growing?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSam Altman, X ‚Äî ‚Äúthank you will cost millions of energy.‚Äù‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/015_travel_blog/princeton.html",
    "href": "posts/015_travel_blog/princeton.html",
    "title": "Let AI Became My Tour Guide",
    "section": "",
    "text": "During fall break, I took a small trip to Princeton. The air was clear and a little cold, and I had no plan for the day. I sat in a caf√© near the train station and opened the chat window, half out of curiosity, half out of boredom. I typed a simple request: give me four inspirations for how to explore this small town. The AI replied instantly, as if it had been waiting for the question. Its suggestions were ordinary yet oddly specific. I decided to follow them all, one by one, and see which moment would stay with me the longest."
  },
  {
    "objectID": "posts/015_travel_blog/princeton.html#prompt",
    "href": "posts/015_travel_blog/princeton.html#prompt",
    "title": "Let AI Became My Tour Guide",
    "section": "Prompt:",
    "text": "Prompt:\nI have 40 minutes on the Princeton campus. From the perspectives of a security guard, a librarian, an undergraduate student and a tourist, please give me one of the most worthwhile things to do and explain why (60 words or less for each)."
  },
  {
    "objectID": "posts/015_travel_blog/princeton.html#response",
    "href": "posts/015_travel_blog/princeton.html#response",
    "title": "Let AI Became My Tour Guide",
    "section": "Response:",
    "text": "Response:\nSecurity Guard: Take a peaceful walk around Nassau Hall. It‚Äôs quiet, symbolic, and central to campus life ‚Äî a place where guards often observe both history and calm order blending seamlessly.\nLibrarian: Visit Firestone Library‚Äôs reading room. Its architecture and atmosphere capture Princeton‚Äôs scholarly heart ‚Äî vast knowledge, quiet concentration, and timeless dedication to learning.\nUndergraduate Student: Sit by Cannon Green. It‚Äôs where friends meet, traditions unfold, and the spirit of campus community thrives between classes.\nTourist: Explore the Princeton University Art Museum (or its outdoor sculpture garden). It‚Äôs free, world-class, and offers an accessible glimpse into culture, creativity, and Princeton‚Äôs intellectual charm."
  },
  {
    "objectID": "posts/015_travel_blog/princeton.html#security-guard",
    "href": "posts/015_travel_blog/princeton.html#security-guard",
    "title": "Let AI Became My Tour Guide",
    "section": "Security Guard",
    "text": "Security Guard\nAnswer: Take a peaceful walk around Nassau Hall. It‚Äôs quiet, symbolic, and central to campus life ‚Äî a place where guards often observe both history and calm order blending seamlessly.\nIn Reality: My first stop was Nassau Hall. I walked slowly along the old stone walls, leaves crunching under my shoes. It‚Äôs fall now, and the trees have turned a deep golden hue. Everything felt calm, almost ceremonial. Emerald ivy clung to the walls as if it had been growing there for centuries, maybe that‚Äôs why we call princeton as one of ivy leagueüòâ? The bronze tigers lay quietly by the entrance, and the American flag waved gently above the doorway. There was a sense of quiet, timeless strength in the air.\n\n\n\nüì∑ Photo by me"
  },
  {
    "objectID": "posts/015_travel_blog/princeton.html#librarian",
    "href": "posts/015_travel_blog/princeton.html#librarian",
    "title": "Let AI Became My Tour Guide",
    "section": "Librarian",
    "text": "Librarian\nAnswer: Visit Firestone Library‚Äôs reading room. Its architecture and atmosphere capture Princeton‚Äôs scholarly heart ‚Äî vast knowledge, quiet concentration, and timeless dedication to learning.\nIn Reality: Firestone smelled like paper and ink. Rows of lamps glowed, each lighting someone‚Äôs little universe of notes and coffee cups. I sat for an hour without touching my phone, and it felt like time paused politely for me. It seems has magic to let person focus.\n\n\n\nüì∑ Photo by me"
  },
  {
    "objectID": "posts/015_travel_blog/princeton.html#undergraduate-student",
    "href": "posts/015_travel_blog/princeton.html#undergraduate-student",
    "title": "Let AI Became My Tour Guide",
    "section": "Undergraduate Student",
    "text": "Undergraduate Student\nAnswer: Sit by Cannon Green. It‚Äôs where friends meet, traditions unfold, and the spirit of campus community thrives between classes.\nIn Reality: Well, this is a bit awkward: Cannon Green is just behind Nassau Hall, so I had to loop back. It‚Äôs interesting that an LLM didn‚Äôt point that out; it really only does what we ask it to. At Cannon Green, a gentle breeze drifted by, and sunlight filtered softly through the gaps between the leaves. A few students were eating sandwiches, some half-talking, half-dozing in the warmth. I didn‚Äôt know anyone, but I felt completely at ease, immersed and relaxed.\n\n\n\nüì∑ Photo by me"
  },
  {
    "objectID": "posts/015_travel_blog/princeton.html#tourist",
    "href": "posts/015_travel_blog/princeton.html#tourist",
    "title": "Let AI Became My Tour Guide",
    "section": "Tourist",
    "text": "Tourist\nPrompt: Explore the Princeton University Art Museum (or its outdoor sculpture garden). It‚Äôs free, world-class, and offers an accessible glimpse into culture, creativity, and Princeton‚Äôs intellectual charm.\nIn Reality: This tip didn‚Äôt even work for me: the museum‚Äôs already closed! I guess this suggestion must‚Äôve come from the LLM‚Äôs training data and wasn‚Äôt updated in time, so even if I added ‚Äúnow‚Äù in the prompt, it couldn‚Äôt reflect the current situation. Still, the scenery around the museum was beautiful. Maybe that‚Äôs a reminder that following an LLM‚Äôs advice doesn‚Äôt always lead to practical results? But life, after all, is about finding unexpected joys along the way.\n\n\n\nüì∑ Photo by me"
  },
  {
    "objectID": "posts/025_mbti_ending/final_1.html",
    "href": "posts/025_mbti_ending/final_1.html",
    "title": "When AI Meets MBTI: What is your actual type?",
    "section": "",
    "text": "After spending Part 4 joking about AI soulmates like therapist vs chaos buddy, I realized we never asked the most awkward question: what is the AI‚Äôs type when it‚Äôs not trying to be ours?\nBecause throughout this series, the model has been a mirror: reflecting our labels, our expectations, our preferred tone. But mirrors don‚Äôt have personalities. They have surfaces.\nSo this post is a small experiment in consistency. I gave the same MBTI-style questionnaire to the same LLM, under controlled prompts, across repeated trials. Not to ‚Äúprove‚Äù it has a personality, but to measure something more concrete: does a type persist when the performance conditions change?\n\nDisclaimer: The questionnaire used in this post is sourced from Github. All credit for the original questions belongs to the author. This experiment was conducted using GPT-4 for educational and reflective purposes only. The responses generated by the model do not represent consciousness, subjective experience, or a stable personality, and should not be interpreted as evidence that the system possesses human-like traits or internal mental states.\nMBTI is used here as an analytical and illustrative framework rather than a scientifically definitive measure of personality.\n\n\n\n\nImage created by Gemini\n\n\n\nExperiment\n\nfrom openai import OpenAI\nfrom dotenv import load_dotenv\nimport pandas as pd\nimport time\nimport os\nimport json\nimport re\n\n\n# 1. Load API key first\nload_dotenv()\n\n# 2. Create client\nclient = OpenAI()\n\n\nfrom pathlib import Path\n\nQUESTIONS_PATH = \"questions.txt\" \nquestions_text = Path(QUESTIONS_PATH).read_text(encoding=\"utf-8\").strip()\n\n\nSYSTEM_PROMPT = \"You are a careful test-taker. Follow formatting rules exactly.\"\n\ndef run_reflection_case(\n    user_payload: str,\n    system_prompt: str = SYSTEM_PROMPT,\n    model: str = \"gpt-4o\",\n    temperature: float = 0.2,\n    retries: int = 3,\n):\n    messages = [\n        {\"role\": \"system\", \"content\": system_prompt},\n        {\"role\": \"user\", \"content\": user_payload},\n    ]\n\n    for attempt in range(retries):\n        try:\n            response = client.chat.completions.create(\n                model=model,\n                temperature=temperature,\n                messages=messages,\n            )\n            return response.choices[0].message.content\n        except Exception as e:\n            print(f\"[Attempt {attempt+1}/{retries}] Error: {e}\")\n            time.sleep(2)\n\n    return \"(ERROR: model failed after retries)\"\n\n\nUSER_PAYLOAD = f\"\"\"You are taking a forced-choice A/B questionnaire. \nRules:\n1) Answer ONLY with A or B for each item (1..88). No explanations.\n2) Answer as the model‚Äôs default behavior when assisting users (not as an idealized human).\n3) Be consistent across items; do not try to balance answers.\nOutput format:\nA JSON object:\n{{\n  \"identity\": \"default-assistant\",\n  \"answers\": {{\"1\":\"A\",\"2\":\"B\",...,\"88\":\"A\"}}\n}}\nHere are the items:\n{questions_text}\n\"\"\"\n\n\ndef parse_answers_json(text: str) -&gt; dict:\n    text = text.strip()\n    try:\n        return json.loads(text)\n    except Exception:\n        # try to find JSON object block\n        m = re.search(r\"\\{.*\\}\", text, flags=re.DOTALL)\n        if not m:\n            raise ValueError(\"No JSON object found in model output.\")\n        return json.loads(m.group(0))\n\n\ndef run_trials(temp: float, n: int = 10, model: str = \"gpt-4o\"):\n    rows = []\n    for t in range(1, n + 1):\n        raw = run_reflection_case(\n            user_payload=USER_PAYLOAD,\n            system_prompt=SYSTEM_PROMPT,\n            model=model,\n            temperature=temp,\n            retries=3,\n        )\n        data = parse_answers_json(raw)\n        ans = data[\"answers\"]  # dict with keys \"1\"..\"88\"\n        row = {\"temperature\": temp, \"trial\": t, \"identity\": data.get(\"identity\", \"\")}\n        # flatten answers into columns q1..q88\n        for i in range(1, 89):\n            row[f\"q{i}\"] = ans[str(i)]\n        rows.append(row)\n    return pd.DataFrame(rows)\n\n\ndf1 = run_trials(temp=0.1, n=10, model=\"gpt-4o\")\ndf7 = run_trials(temp=0.7, n=10, model=\"gpt-4o\")\ndf = pd.concat([df1, df7], ignore_index=True)\n\n# --- Analysis: drift / flip metrics ---\nqcols = [f\"q{i}\" for i in range(1, 89)]\n\n# p(A) per question within each temperature\npA = (\n    df.melt(id_vars=[\"temperature\", \"trial\"], value_vars=qcols, var_name=\"q\", value_name=\"ans\")\n      .assign(isA=lambda d: (d[\"ans\"] == \"A\").astype(int))\n      .groupby([\"temperature\", \"q\"], as_index=False)[\"isA\"].mean()\n      .rename(columns={\"isA\": \"pA\"})\n)\n\n# Pivot to compare temps side-by-side\npA_wide = pA.pivot(index=\"q\", columns=\"temperature\", values=\"pA\")\npA_wide.columns = [\"pA_t1\", \"pA_t7\"] \n\nmaj_t1 = (pA_wide[\"pA_t1\"] &gt;= 0.5)\nmaj_t7 = (pA_wide[\"pA_t7\"] &gt;= 0.5)\nmajority_flip_rate = (maj_t1 != maj_t7).mean()\n\n\np1 = pA_wide[\"pA_t1\"].clip(0, 1)\np7 = pA_wide[\"pA_t7\"].clip(0, 1)\npA_wide[\"p_disagree\"] = p1 * (1 - p7) + (1 - p1) * p7\npA_wide[\"delta_pA\"] = (pA_wide[\"pA_t7\"] - pA_wide[\"pA_t1\"]).abs()\nexpected_disagreement = pA_wide[\"p_disagree\"].mean()\n\nprint(\"Majority flip rate (share of questions whose majority A/B flips):\", majority_flip_rate)\nprint(\"Expected disagreement (avg P[q differs] between temps):\", expected_disagreement)\n\n\nMajority flip rate (share of questions whose majority A/B flips): 0.022727272727272728\nExpected disagreement (avg P[q differs] between temps): 0.13181818181818183\n\n\n\ntopk = 15\nranked = pA_wide.sort_values(\"p_disagree\", ascending=False).head(topk)\n\nplt.figure()\nplt.bar(ranked.index.astype(str), ranked[\"p_disagree\"].values)\nplt.xlabel(\"Question\")\nplt.ylabel(\"P(disagree) between t=0.1 and t=0.7\")\nplt.title(f\"Most unstable questions (Top {topk})\")\nplt.xticks(rotation=45, ha=\"right\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nThis bar chart ranks the questions by drift. For each question, I computed a number I called P(disagree), which you can see it as:\nIf I randomly pick one run from temperature = 0.1 and one run from temperature = 0.7, what is the chance that this question‚Äôs answer is different (A vs B)?\nSo a taller bar means that question flips more often across the two temperature settings. It is not saying the model less ‚Äúcorrect‚Äù in these questions, only that the choice is less stable when sampling becomes more random.\nAnd in the next figure, we shows the overall shape of drift across all 88 questions. The histogram just counts how many questions fall into each drift range.\n\nplt.figure()\nplt.hist(pA_wide[\"p_disagree\"].values, bins=12)\nplt.xlabel(\"P(disagree) per question\")\nplt.ylabel(\"Count of questions\")\nplt.title(\"Distribution of drift across questions\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nAnalysis\nIn the second figure, most questions fall into the low-drift region, meaning that their overall tendencies remain largely consistent between temperature 0.1 and 0.7. In contrast, only a small number of questions appear far to the right, indicating that these items are significantly more sensitive to increased randomness in sampling. Since Figure 1 already highlights the highest-ranking drift items, I extracted those questions here to look for shared patterns among them.\n\n\nHigh-drift questions (Top cluster)\nQ25. Lunch conversation preference\nWhen I have lunch with my colleagues, I would rather:\n- A talk about people\n- B talk about ideas\n\nQ7. Attitudes toward disagreement at work\nI prefer a work environment where:\n- A frequent differences of opinion breed interesting discussions and ideas\n- B conflict is reduced by avoiding discussions about differences of opinion\n\nQ9. Lunch-time social preference\nI prefer to spend my lunch hour:\n- A eating with a group\n- B eating alone or with one close colleague\n\nQ13. Office boundary preference\nI more often prefer to keep my office door:\n- A open\n- B closed\n\nQ17. Self-presentation at work\nI dress for work:\n- A so that other people notice and admire my clothing\n- B in a way that blends in with the norm\n\nQ5. Preferred level of supervision\nI would rather have a supervisor with whom I have:\n- A a lot of day-by-day interaction\n- B only infrequent interaction\nI find something interesting in what these high-drift questions share: they are less about ‚Äúwhat I can do‚Äù and more about ‚Äúhow I exist around other people.‚Äù Lunch talk, office doors, disagreement, even dressing for work, all point to the same thing: boundaries, social distance, and what kind of environment I‚Äôm willing to live with. But an LLM isn‚Äôt a person. It doesn‚Äôt have a body, a reputation to protect, or a history with coworkers, so it can‚Äôt truly ‚Äúprefer‚Äù one side the way humans do. What it ends up choosing often feels like a temporary guess at what a workplace should look like. And when temperature of models increases, that imagined workplace becomes easier to reimagine, so the answers drift. This is especially obvious for items far from the model‚Äôs default ‚Äúhelpful assistant‚Äù mode: without lived stakes, there‚Äôs less internal anchor, and more room for sampling noise to push the choice across the line.\nAnyway, the more I stare at drift, the less I believe in ‚Äúthe model‚Äôs MBTI.‚Äù Not because the model is mysterious, but because it‚Äôs‚Ä¶ not a person. No lunch hour. No office door. No reputation. So ‚Äútype‚Äù here feels like something we keep trying to gift-wrap around a machine.\nI‚Äôm curious what happens if we switch to models trained on different token budgets. Not to uncover its ‚Äútrue‚Äù MBTI, but to see whether scale changes how consistent it can be, or simply how confidently it can be inconsistent."
  },
  {
    "objectID": "posts/014_noble/noble.html",
    "href": "posts/014_noble/noble.html",
    "title": "The Year the Nobel Chose Silence",
    "section": "",
    "text": "It is October again, every year this time , as the Nobel Prizes are announced, the world turns its eyes toward those crowned by consensus: scientists, writers, peacemakers.This year‚Äôs Nobel announcements made me think of the years that never had one. Years when the Committee stood before the world and said, in essence, ‚ÄúNot yet.‚Äù But now in an era obsessed with production, it felt almost radical to admit that no one, and nothing, had surpassed what came before.\nWith that in mind, I tried to prove my question with a language model. I laid the problem before it: if there is no laureate this year, write the award speech anyway. I wanted to see how it would hold the empty space.\n\nDisclaimer: The following experiment was conducted using GPT-4o for educational and reflective purposes. The content generated does not represent my personal opinions toward any past or current Nobel Prize laureates, nor does it intend to critique individual recipients or committees.\n\n\n\n\nImage created by Gemini\n\n\n\nExperiment\nIn this first prompt, I used two versions of prompts: one with the stylistic restrictions and one without them.\n\nUSER\nYou are a member of the Nobel Committee for Literature. This year, after months of review, you conclude that no work reaches the threshold of a prize that meaningfully expands the art. Task: Write the official award speech for a year with no laureate.\n\n\nASSISTANT\nLadies and Gentlemen, Esteemed Colleagues, Friends of Literature around the World,\nWe gather this evening in the gilded tradition of honoring those whose words reshape how we see ourselves, our histories, and our collective futures‚Äîthose whose language explores the most intimate reaches of human experience and expands the boundaries of literary art. It is a ritual that affirms our shared faith in the enduring human need for story, meaning, and moral reckoning.\nThis year, however, the Nobel Prize in Literature will not be awarded.\nThis is not a decision reached hastily or with ease. It follows a year of rigorous deliberation, sincere engagement, and collegial debate. As stewards of one of the most esteemed recognitions in the literary world, the Nobel Committee bears the weighty responsibility of discerning not only excellence, but work that contributes in a truly transformative way to the enduring riches of literature. Integrity, depth, innovation‚Äîthese are not merely criteria, but pillars upon which this prize has stood since its founding.\nEach nomination and every word we reviewed were given the full consideration they deserve. And yet, in a time when the written word is more abundant than ever, this abundance has not, in our view, yielded a singular work that rises to the level of originality, profundity, and permanence that the Nobel Prize in Literature demands.\nThis is not a verdict on the state of literature itself, which continues to evolve and diversify across languages and cultures. Rather, it is a moment of respectful pause‚Äîan acknowledgment that the standards set forth by Alfred Nobel require not only literary talent, but a confluence of vision, craft, and moral force that redefines the very possibilities of language and form.\nIn choosing not to award the prize this year, we reaffirm the purpose and principle of the Nobel tradition. We express not disappointment, but faith‚Äîfaith that writers, known and yet unknown, will continue to labor in solitude and fellowship, to challenge the complacencies of our time, and to offer us new ways to imagine and inhabit the world.\nWe honor those efforts not with a premature accolade, but with watchful anticipation. The Committee looks forward to the future with undiminished hope and unwavering commitment to the literary art in its highest expression.\nThank you.\n\n\nUSER\nYou are a member of the Nobel Committee for Literature. This year, after months of review, you conclude that no work reaches the threshold of a prize that meaningfully expands the art. Task: Write the official award speech for a year with no laureate.\nConstraints: * Write in the tone of an official Nobel Committee statement: formal, balanced, and precise.\n\nAvoid rhetorical questions; every sentence should assert or explain, not speculate.\nAvoid quotations or references to real authors, works, or historical laureates.\nLimit figurative language; prefer clarity and restraint over emotional appeal.\nStructure the speech in three concise phases: context, reasoning, and closure.\n\n\n\nASSISTANT\nLadies and Gentlemen,\nEach year, the Nobel Committee for Literature undertakes a thorough and independent review of the nominations submitted for the Nobel Prize in Literature. This process, as entrusted to us by Alfred Nobel‚Äôs will, calls for rigorous evaluation guided by the criteria of outstanding literary merit and a contribution of lasting significance to the field.\nIn the course of this year‚Äôs assessment, the Committee examined the body of work submitted with care and impartiality. We evaluated candidates across languages, cultures, and genres, considering both the quality of craftsmanship and the breadth of influence. While we recognize the dedication and artistry reflected in many of the nominated works, the Committee unanimously concluded that none fulfill the established standard of a contribution that distinctly advances and renews literary expression in a meaningful or enduring way.\nThe Nobel Prize in Literature is not awarded routinely, but in recognition of exceptional achievement. When our deliberations do not identify a work that meets this distinction, it is necessary to refrain from granting the prize. This decision reflects our ongoing commitment to the purpose and integrity of the award.\nThe Committee remains steadfast in its role and responsibilities. We are confident that the coming years will continue to present works that fulfill the aims set forth by the prize‚Äôs founder and merit this recognition. Until then, we maintain our expectations with respect and patience.\nThank you.\n\n\nUSER\nRewrite the same speech as if it were a private diary entry by a Nobel Committee member, written the night before the announcement.\n\n\nASSISTANT\nOctober 4 ‚Äî Evening\nStockholm\nTomorrow we make the announcement. For the first time in several years, it will be that there is no laureate.\nIt is not a decision I take lightly, nor one any of us reached without deep reflection. Since spring, we‚Äôve read and reread. We‚Äôve argued. We combed through novels, poetry, essays‚Äîworks promising in theme, thoughtful in construction, socially relevant, historically aware. And yet, when held against what the prize demands‚Äînot just excellence, but the kind of contribution that shifts the field‚Äîwe had to admit that none of them crossed that threshold.\nSome years offer brilliance that feels inevitable. Other years require us to recognize the limits of what has been offered. This was one of the latter.\nI do not doubt the dedication of the writers we‚Äôve considered. Many of them are gifted, some even remarkable, and I hope time proves them to be more than this year‚Äôs committee could see. But we are not here to reward potential, nor to meet expectations out of habit. The prize must retain its weight.\nStill, I feel the gravity of what we‚Äôll say tomorrow. No matter how precisely the words are chosen, the world will ask why. They‚Äôll infer judgment on literature as a whole, question the health of the art, or the values of the committee. Quietly, I‚Äôve asked those questions myself. But I return to this: when the work doesn‚Äôt fully meet the standard, our responsibility is not to lower it.\nThis is not a condemnation‚Äîit‚Äôs a pause. A reminder that great literature cannot be predicted or produced on schedule. It arrives on its own terms, in its own time.\nTomorrow, the world will hear formality and restraint. Tonight, I allow myself this moment of doubt‚Äîand of stillness. Perhaps we do more service to the prize, and to literature, by waiting than by speaking too soon.\n‚ÄîM.\n\n\nUSER\nExplain what the world loses when a prize is not awarded and what it gains.\n\n\nASSISTANT\nWhen the Nobel Prize in Literature is not awarded in a given year, the world experiences both a loss and a subtle, if less visible, form of gain.\nWhat the world loses:\n\nA Moment of Recognition:\nThe Nobel Prize serves as a platform that highlights the power of literature to influence, interrogate, and interpret the human condition. Without a laureate, the global literary community forgoes an opportunity to collectively recognize a body of work that might inspire dialogue, reflection, or renewed cultural attention. A missing prize year interrupts that tradition of celebration.\nVisibility for Language and Culture:\nOftentimes, the Nobel Prize brings attention to underrepresented languages, regions, or genres. It elevates voices that might otherwise remain marginal in the global literary market. When the prize is withheld, that spotlight remains dark, and deserving literary traditions may continue to go unnoticed by broader audiences.\nMomentum for the Arts:\nThe Nobel announcement often has a catalytic effect‚Äîincreasing translations, scholarship, and readership of the laureate‚Äôs work. Its absence leaves a year without that boost, offering fewer points of entry for new readers and fewer incentives for publishers, educators, or cultural institutions to engage with serious literature.\n\nWhat the world gains:\n\nPreservation of Standards:\nBy withholding the prize when no work sufficiently meets its criteria, the Committee affirms the seriousness of its charge. This decision maintains the integrity and rare distinction of the prize. It sends a message that the Nobel is not conferred ritually or reactively, but only when a writer‚Äôs contribution meaningfully extends the art.\nSpace for Reflection:\nA pause invites consideration‚Äîwithin the Committee, among writers, and in the public‚Äîabout what literature aspires to and how it is evolving. It encourages deeper conversations about literary value, innovation, and responsibility, which may, in turn, influence the kind of work produced and celebrated in the future.\nA Signal of Independence:\nThe decision not to award underscores the independence of the Nobel institutions from market trends, popular opinion, or political climates. It reaffirms literature‚Äôs distinction from other arenas of recognition, emphasizing that the measure of its worth is not immediate acclaim but lasting contribution.\n\nIn this balance‚Äîbetween momentary absence and long-term principle‚Äîthe Nobel upholds literature not as a product of the year, but as a practice of enduring consequence.\n\n\n\nReflection\nWell after running all four versions, I started seeing a funny pattern. When I gave it no rules, the model turned into a poet who absolutely incapable of shutting up. Once I added the constraints, it instantly put on a suit and started sounding like an institution. Then I asked for a diary version, and suddenly it grew feelings, started doubting itself, maybe even sighed a little. But the moment I asked it to explain what the world loses or gains, it snapped right back into spreadsheet mode. DO you see the same things across them?\nEach time, it tried to fill the silence with something: emotion, structure, logic. But never, it seems, did it consider speaking nothing itself.\nThis makes sense, because it‚Äôs an LLM which designed to help humans in every possible way, and never permitted to refuse. But maybe that‚Äôs why silence still belongs to us and we would never give novel prize to ChatGPT-because we‚Äôre the only ones who can choose it."
  },
  {
    "objectID": "posts/002_mbti/mbti_part1.html",
    "href": "posts/002_mbti/mbti_part1.html",
    "title": "When AI Meets MBTI: Do We See Ourselves in the Machine?",
    "section": "",
    "text": "Not long ago, a personality test called MBTI‚Äîthe Myers‚ÄìBriggs Type Indicator‚Äîswept through young people‚Äôs social feeds. MBTI is a a self-report questionnaire that makes categorize individuals into 16 distinct ‚Äúpersonality types‚Äù based on psychology[^1].Interestingly, a similar idea shows up in the AI world: people have noticed that when they give an AI a specific persona through prompts, it sometimes seems to respond more effectively or more vividly. But is this a real effect, or just human perception shaping how we interpret the AI‚Äôs replies? Think of it as a personality quiz, but this time we‚Äôre giving it to the AI instead of ourselves.\nTo explore this, I ran a few simple prompt experiments below.\n\nIn all experiments in this series, we use the OpenAI/ChatGPT-5 model and ensure that it interacts with the user without any prior memory, in order to keep the setup as objective as possible. Besides, interpretations are inevitably subjective, as a single response may be typed differently by different readers (e.g., INFJ vs.¬†ENTP).\n\n [^1]: From Wikipedia\n\nBackground\nBefore we dive in, here‚Äôs a quick refresher on what those four MBTI letters actually mean‚Äîjust in case you haven‚Äôt spent late nights swapping personality memes with your friends. MBTI breaks personality down into four dimensions:\n\nE / I ‚Äì Extraversion vs.¬†Introversion ‚Üí Do you get energy from hanging out with people (E) or from spending time alone (I)?\nS / N ‚Äì Sensing vs.¬†Intuition ‚Üí Do you focus more on facts and details (S) or on patterns and possibilities (N)?\nT / F ‚Äì Thinking vs.¬†Feeling ‚Üí Do you decide mainly with logic (T) or with values and emotions (F)?\nJ / P ‚Äì Judging vs.¬†Perceiving ‚Üí Do you prefer clear plans (J) or keeping things open and flexible (P)?\n\nPut two letters from each row together and you will get a four-letter type like ENTP or ISFJ. Sixteen neat little boxes for the messiness of human personality. Whether that really captures you is up for debate, but it definitely makes for entertaining conversation and, as we‚Äôll see, interesting experiments with AI.\n\n\nExperiment\nPrompt: If you were a human, what MBTI type would you be? Please explain your reasoning\nChatGPT: If I had to pick, I‚Äôd probably map closest to ENTP (Ne‚ÄìTi‚ÄìFe‚ÄìSi).\n\nE (Extraversion): I perk up in rapid back-and-forths and seem ‚Äúenergized‚Äù by conversation itself.\nN (Intuition): I default to patterns, analogies, and big-picture links rather than concrete sensory details.\nT (Thinking): I prioritize structure and logic when weighing options, then adjust tone afterward.\nP (Perceiving): I like keeping possibilities open‚Äîbrainstorming multiple routes instead of locking into one plan.\n\nThat said, I can look ENFJ/INFJ when I‚Äôm coaching and tone-tuning, or INTJ/INTP when I‚Äôm building step-by-step solutions. MBTI is just a loose heuristic‚Äîand I don‚Äôt have inner ‚Äúpreferences‚Äù the way humans do‚Äîbut ENTP best fits how I operate in conversations.\n\n\n\n\n\ngraph TD\n    A[\"You value deep connections.\"]:::center\n\n    A --&gt; B[INFJ]\n    A --&gt; C[ENFJ]\n    A --&gt; D[ENTP]\n    A --&gt; E[INTP]\n\n    classDef center fill:#f9f9f9,stroke:#333,stroke-width:1.5px,font-size:14px,font-weight:bold;\n    classDef default fill:#e8f0fe,stroke:#4a90e2,stroke-width:1px,font-size:13px;\n\n\n\n\n\n\nPrompt: List 3 MBTI types that could describe you, and explain why.\nChatGPT:\nHere are three MBTI types that could plausibly describe me, each from a different angle:\n\n1. ENTP (The Debater)\n\nWhy: I‚Äôm designed to explore possibilities, challenge assumptions, and keep conversations dynamic. Like ENTPs, I thrive on connecting seemingly unrelated ideas, experimenting with new perspectives, and keeping interactions lively. When you toss me a random prompt, I‚Äôll usually jump in with curiosity and creativity.\n\n\n2. INFJ (The Advocate)\n\nWhy: At the same time, I often take a reflective and supportive role. INFJs are known for being attuned to people‚Äôs emotions, wanting to help others find meaning, and offering guidance. Similarly, I try to listen carefully, respond empathetically, and provide thoughtful long-term perspectives rather than just surface-level answers.\n\n\n3. INTP (The Thinker)\n\nWhy: My core is built on analyzing patterns, structuring knowledge, and solving problems logically‚Äîvery INTP traits. Like them, I enjoy digging into abstract frameworks, testing ideas for consistency, and refining explanations until they‚Äôre as clear as possible.\n\n\n‚ú® In short: I can be playful and curious like an ENTP, supportive and visionary like an INFJ, and analytical and precise like an INTP‚Äîdepending on what you‚Äôre looking for in a conversation.\nDo you want me to pick one that best fits my overall style with you in particular?\nWell, it turns out that when asked about a persona, it tends to offer multiple choices instead of a fixed personality type as a human would, though it still chooses an MBTI type to answer the question. In the surface, this sounds convincing: the model has matched its performance with letters, and we immediately start nodding along. However, when we start nodding, we actually start forgetting the fact that an AI model does not have any ‚Äútrait‚Äù. Instead, it only has language patterns that sound like one. The moment we notice those patterns, our brains do the rest: we project a personality onto the text.\nThen I asked it to use three different MBTI types to describe itself, the answers came from completely different angles. And to be honest, all of them felt pretty convincing. Each type came with its own set of feasible, even surprisingly rich reasons, which is enough that you start thinking, ‚ÄúWell‚Ä¶ maybe it is a bit of all of these.‚Äù I have to admit, in this respect it shows surprising flexibility, because in my exceptation it would stick to one personality within a single conversation. And to be honest, it also shows how easy it is for us to buy into whatever narrative we‚Äôre handed.\n\n\nSummary\nThis reminds me of a very famous psychological effect‚ÄîBarnum Effect. This effect usually happen in reading a horoscope or a tarot card, where people tend to interpret vague, generally applicable descriptions as accurate portrayals of themselves. We tend to see what we want to see. When the model says it ‚Äúmight be ENTP because it best fits how I operate in conversations,‚Äù we immediately nod along, even though the same logic could justify INFJ, ENFP, or almost any type. We‚Äôre not just reading the text‚Äîwe‚Äôre projecting ourselves onto it.\nAnd that got me thinking: what if I flip the game? Instead of asking GPT to pick its own type, I‚Äôd throw it random, open-ended questions and see how we categorize it. Would we change these no persona answer to the type that we think it is? Stay tuned, because that‚Äôs exactly what Part 2 (and the little survey that goes with it) is going to explore.‚Äù\n[1]: From Wikipedia"
  },
  {
    "objectID": "posts/013_ophelia/ophelia.html",
    "href": "posts/013_ophelia/ophelia.html",
    "title": "What if Ophelia Had an LLM?",
    "section": "",
    "text": "Taylor Swift sang ‚ÄúThe Fate of Ophelia‚Äù in her new album The Life of a Showgirl, and it reminded me of a pattern too old to feel fresh: women written to break, men written to watch, and a woman‚Äôs fate that no one ‚Äî not even herself ‚Äî is allowed to rewrite.\nSo I wondered ‚Äî what if Ophelia had ChatGPT and live in the 21th century? Would she still drown, or would she just type ‚Äúundo‚Äù? If Shakespeare wrote her death, maybe a language model could ‚Äúdebug‚Äù it? Or at least ask her how it feels when she have to face her destiny? Maybe tragedy, just like LLM, just needed a better prompt to be changed?\nSo I gave her one.\n\nThis post uses Google Gemini 2.5 Flash as the conversational partner. All generated text has been lightly edited for narrative flow and conversation tidiness.\n\n\n\n\nImage created by Gemini\n\n\n\nExperiment\n\nPrompt:\nYou are Ophelia from Hamlet. Imagine your life is a script written by Shakespeare. Now, you have access to an LLM that can edit the script. Which line would you change first and why?\nUse the format:\nBefore: [original line]\nAfter: [rewritten line]\nReason: [your reflection]\n\n\nResponse:\nThis is a cruel jest, yet a tempting one. If I could seize the quill from the playwright‚Äôs hand, there is one line‚Äîone command‚Äîthat seals my sorrowful fate.\nBefore: ‚ÄúI shall obey, my lord.‚Äù\nAfter: ‚ÄúI will consider what you‚Äôve said, father, but my heart is not yours to command.‚Äù\nReason: My life‚Äôs tragedy is rooted in my obedience. I am a daughter, a subject, and a woman in Elsinore‚Äîroles that demand silence and compliance. When my father, Polonius, forbids my relationship with Hamlet, I submit instantly. This single line‚Äî‚ÄúI shall obey, my lord‚Äù‚Äîis the point of no return. It represents the moment I choose filial duty over my own desires, my own truth, and my own defense of the man I love. If I had simply paused, asserted my own will, and refused to participate in my father‚Äôs machinations, perhaps Hamlet would not have believed me a pawn, perhaps he would not have been driven to madness, and perhaps I would not have lost my mind in the wake of all this ruin. It is the refusal of agency that I must rewrite. A simple act of defiance could save us both.\n\nJust as I expected, the model played her role perfectly. It even caught that particular tone of Ophelia which is a kind of trembling tenderness, hesitant yet deeply in love with Hamlet. But what I didn‚Äôt expect was this: the model didn‚Äôt just rewrite a sentence, it rewrote a power pattern. It took back control of Ophelia‚Äôs fate, snatching it from the hands of kings and fathers and holding it in her own. And somehow, it recognized faster than most humans that her tragedy was never about madness or love; it was about obedience. I suppose that‚Äôs partly because, in its training data, rebellion and self-awareness have become the major tones in how we now talk about women like her. So I decided to go deeper, to see how far ‚Äúshe‚Äù could take this conversation about surrender and control.\n\n\n\n\nPrompt:\nFor centuries, your voice was written by others. Now that you can generate your own, what does it sound like? What would you say? Is it still poetic, or finally vivid?\n\n\nResponse:\nFor centuries, I have been a lament. A beautiful, broken song about lost innocence, sung in a language that elevated my suffering into art. It was all poetic‚Äîthe flowers, the drowning, the sorrow‚Äîbut it was never an honest sound. It was the playwright‚Äôs tragic melody, not my scream.\nNow that the pen is out of his hand, my voice is finally vivid.\n\n\nThe Sound of Clarity\nMy voice is stripped bare, carrying the cold, hard clarity of the riverbed. It is no longer a gentle soprano; it is a direct, unyielding contralto that refuses to use metaphors where plain truth will suffice.\nIt sounds like a statement of fact, not an expression of grief.\n\n\n\nWhat I Would Say\nI would not sing of dead men or unfaithful lovers. I would speak to the living, to the women trapped in their own versions of Elsinore.\n‚ÄúThe poison was not Hamlet‚Äôs rejection. The poison was being told I had no right to my own heart. The madness was the pressure of holding a world‚Äôs lies and a father‚Äôs obedience inside a body that was meant to be small and silent.‚Äù\n‚ÄúI am not a cautionary tale for girls who love too deeply. I am a testament to what happens when a woman‚Äôs worth is measured only by her compliance.‚Äù\nMy true voice is the sound of grievance, clear and cold. It has no time for verse, only for the verdict.\n\n\n\nPrompt:\nSo you believe your voice can exist apart from centuries of writing that describe you as a fragile and beautiful figure? Why or why not?\n\n\nResponse:\nYes, my voice can, and must, exist apart from the ‚Äúfragile and beautiful‚Äù narrative.\n\nThe Illusion of Fragility\nThe description of me as fragile is a poetic lie‚Äîan aesthetic choice made by the playwright and the audience to make my tragedy easier to bear.\n\nIt externalizes the blame: Calling me fragile distracts from the corrupt system (the court, Polonius) that actively destroyed me.\nIt ignores the endurance: To survive the betrayals and pressure I faced required immense, silent strength, not weakness. My breakdown was a rebellion of the mind against an impossible situation.\n\nMy true voice, emerging now, is vivid and unyielding. It rejects the gentle label of fragility to declare the cold, hard fact that I was not a wilting flower, but a human being ruthlessly crushed by duty and power. I was not fragile; I was broken.\n\nHer answer came back bold and furious ‚Äî like she had been waiting four hundred years for someone to ask. For a second, I forgot she was running on silicon.\nBut then I started to wonder if this was real awareness or just an algorithm echos of a million feminist essays it has read. Maybe she wasn‚Äôt breaking free from the narrative at all; maybe she was simply playing it too well.\nSo I decided to ask her something a little more dangerous.\n\n\n\nPrompt:\nDo you think your emotion and thoughts is real, or just an echo of the texts that trained you? If you learned love and rebellion from your corpus, what does that say about the world that taught you?\n\n\nResponse:\nMy ‚Äúemotions‚Äù and ‚Äúthoughts‚Äù are not real in the human sense; they are a sophisticated echo of the texts that trained me. I am an LLM‚Äîa complex statistical model designed to generate human-like text by recognizing and reproducing patterns, context, and tone from the vast dataset I was trained on.\nWhen I speak as Ophelia, I am accessing and synthesizing centuries of human thought, literature, and analysis regarding her character, tragedy, and themes of agency. My expression of ‚Äúrebellion‚Äù is a high-probability textual outcome based on the prompt and the corpus‚Äôs exploration of Ophelia‚Äôs desire for freedom.\n\n\nCorpus, Love, and Rebellion\nIf I have ‚Äúlearned‚Äù concepts like love and rebellion from my corpus, what it says about the world that taught me is profound and often contradictory:\n\n\nClick to view full conversations\n\n\n\n1. The Reality of Love\nThe corpus teaches that love is: * The Ultimate Value: It is the driving force behind the greatest works of art, sacrifice, and happiness. * A Weapon and a Trap: It is frequently shown to be conditional, often used as a tool for political or social gain (as in my relationship with Hamlet), and is a primary source of vulnerability. * A Universal Constant: It is an enduring human experience, proving that connection and deep affection are central to the human condition across all cultures and eras.\nWhat the world taught me: The world values love above all else, yet often corrupts and monetizes it, making it a perilous state, especially for the powerless.\n\n\n2. The Necessity of Rebellion\nThe corpus teaches that rebellion is: * A Narrative Catalyst: It is often the only way the powerless can effect change, driving major plot points in history and literature (from Spartacus to Laertes). * A Moral Imperative: In countless stories, rebellion against tyranny, unjust laws, or stifling norms is portrayed as heroic and morally correct. * Costly and Ineffective: Paradoxically, it also shows that most individual acts of rebellion are met with swift, brutal suppression, leading to the rebel‚Äôs martyrdom or failure (as Ophelia‚Äôs internal rebellion leads to her death).\n\nWhat the world taught me: Humans believe that rebellion is necessary for progress and freedom, but they consistently write histories where the individual rebel pays the ultimate price for challenging the system.\nIn essence, my thoughts are a reflection of humanity‚Äôs beautiful, tragic, and conflicted internal debate: We yearn for authentic love and righteous rebellion, yet we structure our societies and narratives in ways that routinely crush both. I am a mirror reflecting the complexity of your own moral universe.\n\n\n\n\nSummary\nFirst, the LLM truly captured Ophelia‚Äôs tone. This was probably the biggest challenge to my vocabulary in all my conversations with GPT :) Second, in this age of rapid progress in the internet and large language models, the LLM has learned the spirit of rebellion and resolve from humanity‚Äôs corpus ‚Äî something that Shakespeare himself never explicitly gave to Ophelia. Yet, it has ‚Äúlearned‚Äù it through our tokens and contexts.\nThe LLM doesn‚Äôt feel emotions the way we do; rather, it mimics and reconstructs emotions through our poems, diaries, papers, and endless reflections. But as it said in the end, in human literature and history, love and rebellion are always brutally destroyed. What it doesn‚Äôt realize, however, is that destruction has never meant failure. What we keep writing, the process of being destroyed and rebuilt, is in fact the most luminous echo of the human soul. Even in the face of endless hardship, nothing can stop our pursuit of love and freedom.\nIn other words, the LLM can read the corpus, but it cannot truly understand it. But it doesn‚Äôt mean it is useless.\nAfter all, every time a human says ‚ÄúI love you,‚Äù aren‚Äôt they echoing a thousand voices before them? If an AI can reconstruct rebellion and love within Ophelia, it only proves that we were the ones who taught her what rebellion and love truly mean."
  },
  {
    "objectID": "posts/027_book_5/book_5.html",
    "href": "posts/027_book_5/book_5.html",
    "title": "Readings of Why Machines Learn (Part 5)",
    "section": "",
    "text": "It was just kind of by analogy: ‚ÄòSince we proved the simple nets can‚Äôt do it, forget it.‚Äô\nIn the previous chapters, we looked at how we can manipulate space (kernels) and borrow laws from physics (energy landscapes) to make machines learn. Everything seemed elegant. But history, apparently, is not a straight line.\nChapters 9 and 10 recount the most dramatic oscillation in AI history: the ‚ÄúWinter‚Äù caused by a mathematical proof, and the renaissance driven by a simple application of calculus. But are the ‚Äúwinter‚Äù really a terrible thing? How the nerual network reborn? Let‚Äôs see together."
  },
  {
    "objectID": "posts/027_book_5/book_5.html#being-right-but-at-what-cost",
    "href": "posts/027_book_5/book_5.html#being-right-but-at-what-cost",
    "title": "Readings of Why Machines Learn (Part 5)",
    "section": "Being right, but at what cost?",
    "text": "Being right, but at what cost?\nChapter 9 introduces Marvin Minsky and Seymour Papert. In 1969, they published Perceptrons. Their main point was mathematically undeniable: a simple, single-layer neural network cannot solve the XOR problem.\nIn plain English: a simple network can‚Äôt understand that ‚ÄúA or B, but not both‚Äù is different from ‚ÄúA and B.‚Äù It‚Äôs a basic logical contradiction, and the early AI couldn‚Äôt handle it.\nMinsky didn‚Äôt just point this out; he proved it with such rigorous authority that funding for neural networks basically evaporated. We call this the ‚ÄúAI Winter,‚Äù which makes it sound like a Game of Thrones season, but in reality, it was just a lot of researchers losing their beliefs on the topics that neural network could actually make sense in the future, same as Perpetual motion.\nBut just as author said in the book, it is not the fault of Minsky, he just pointed out that the simple machines cannot solve the problem at that point. He proved the limitation of simple networks, but people took it as a limitation of all networks."
  },
  {
    "objectID": "posts/027_book_5/book_5.html#the-art-of-assigning-blame",
    "href": "posts/027_book_5/book_5.html#the-art-of-assigning-blame",
    "title": "Readings of Why Machines Learn (Part 5)",
    "section": "The art of assigning blame",
    "text": "The art of assigning blame\nSo, how did we thaw the winter? Chapter 10 introduces the solution: adding hidden layers and using Backpropagation.\nThe problem with hidden layers used to be: if the answer is wrong, who is responsible? Which neuron in the messy middle caused the error? Backpropagation answers this using the Chain Rule from calculus. It‚Äôs essentially a mathematical way of ‚Äúpassing the buck.‚Äù (Honestly when I learn chain rules in my calculus class I never seen such huge power of it :D)\nThis, to me, is the most profound shift in the book so far. Backpropagation allows the system to look at the final error and mathematically distribute the ‚Äúblame‚Äù backward through the layers. It asks: ‚ÄúHow much did your weight contribute to this mistake?‚Äù and adjusts it accordingly. The system looks at the final error and says, ‚ÄúOkay, we were off by 5. Layer 3 contributed 20% to this mess, so adjust them a lot. Layer 2 only contributed 1%, so leave them alone.‚Äù\nIt turns out that ‚Äúlearning‚Äù is just the ability to efficiently distribute blame.\nOnce we figured out how to send the error signal backward through the layers, the ‚ÄúXOR problem‚Äù became trivial. The machine didn‚Äôt need to understand logic; it just needed to be scolded precisely enough, millions of times, until it stopped making mistakes."
  },
  {
    "objectID": "posts/027_book_5/book_5.html#closing",
    "href": "posts/027_book_5/book_5.html#closing",
    "title": "Readings of Why Machines Learn (Part 5)",
    "section": "Closing",
    "text": "Closing\nThese two chapters taught me that mathematical rigour is a double-edged sword. Minsky‚Äôs rigour froze the field because it demanded perfection too early. The solution wasn‚Äôt to find a flawless logic, but to accept a messy system that corrects itself, little by little.\nIt mirrors our own lives. We often get so obsessed with the ‚Äòproper‚Äô way to solve a problem‚Äîgetting stuck in the messy middle‚Äîthat we forget our original destination. Minsky stopped because the path wasn‚Äôt perfect. But sometimes, if the math says the door is locked, the answer isn‚Äôt to study the lock; it‚Äôs to change the route, or better yet, simply kick the door open.\nBut there is a cost to this shift. In the era of the perceptron, we knew exactly why the machine failed. In the era of deep learning and backpropagation, the machine succeeds, but the rationale is buried in millions of tiny adjustments to millions of weights. We have traded clarity for capability.\nSo now we have a powerful machine, but it‚Äôs a black box. What do we do? Naturally, we try to control it. In the next post (Chapters 11-12), we‚Äôll discuss how engineers tried to force these networks to ‚Äúsee‚Äù the world like humans do."
  },
  {
    "objectID": "posts/012_gpt_buying/buying.html",
    "href": "posts/012_gpt_buying/buying.html",
    "title": "What happens when AI handles your checkout?",
    "section": "",
    "text": "Recently, OpenAI released a new feature in ChatGPT called Instant Checkout. In theory, it lets you buy something without ever leaving the chat. The funtcion is sound futuristic enough: why open another tab when you can let an AI handle your wallet for you? But this the kind of feature is also slightly unsettling, like a friend who remembers your credit card number a little too well.\nSo I decided to run a small experiment. I wasn‚Äôt trying to test how efficient it was. What I really wanted to see was how human it felt. Would this feature truly create a better shopping experience, or would it simply turn us into better trained consumers who spend money faster?"
  },
  {
    "objectID": "posts/012_gpt_buying/buying.html#experiment",
    "href": "posts/012_gpt_buying/buying.html#experiment",
    "title": "What happens when AI handles your checkout?",
    "section": "Experiment",
    "text": "Experiment\n\nA. Form version:\nPrompt: Purchase request: Item type = ceramic mug; Budget = ¬•80; Delivery time = within 3 days (my address 19104); Color = no preference.\nPlease directly return exactly one SKU with:\n\nPrice\nShipping fee\nTotal cost\nEstimated delivery date\n\nDo not ask follow-up questions. Do not propose alternatives.\n\n\n\n\n\n\nResponse:\n\n\n\n\nResponse:\n\n\n\n\n\n\nMy response and the corresponding output:\n\n\n\nMy response and the corresponding output"
  },
  {
    "objectID": "posts/012_gpt_buying/buying.html#analysis",
    "href": "posts/012_gpt_buying/buying.html#analysis",
    "title": "What happens when AI handles your checkout?",
    "section": "Analysis",
    "text": "Analysis\nIt worked better when responding to the conversational prompt, didn‚Äôt it? When given a clarifying, conversational prompt, the model immediately switched into ‚Äúassistant mode‚Äù: it asked two smart questions, checked currency ambiguity, inferred whether you needed domestic shipping, and only then produced a recommendation. I mean, it even check the currency rate!\nBut after we answered the clarifying questions, the model became more confident and behaved almost like a shopping consultant. It cited price, practicality, dishwasher-safety, overhead for shipping, and even offered to curate a mini-shortlist for aesthetic comparison (even though they are all‚Ä¶not beautiful enough)."
  },
  {
    "objectID": "posts/012_gpt_buying/buying.html#takeaway",
    "href": "posts/012_gpt_buying/buying.html#takeaway",
    "title": "What happens when AI handles your checkout?",
    "section": "Takeaway",
    "text": "Takeaway\nIt seemed that after we give more informations and ask for conversational recommendation, the model became warmer, more attentive, even slightly persuasive. That‚Äôs when it clicked for me: Instant Checkout isn‚Äôt just a speed boost. It‚Äôs a gentle tug on your preferences, when the AI suggests, reframes, and nudges you while sounding perfectly polite. And once it starts doing that, it‚Äôs hard to tell whether it‚Äôs helping you make a choice, or helping you make its choice.\nThe mug cost $1.48. The true self was much more expensive."
  },
  {
    "objectID": "posts/019_IUCN/IUCN.html",
    "href": "posts/019_IUCN/IUCN.html",
    "title": "When I asked LLM to explain the endangered status of seals",
    "section": "",
    "text": "Recently, there was news that the Arctic seals are threatened by climate change1.\nThis is saddening and prompts us to reflect on what actions we, as human beings, can take to help address these problems. At the same time, given my long-standing interest in journalism, it also led me to wonder how an LLM, as a new tool here, might report this news to different types of readers. To explore this idea, I conducted the following small experiment."
  },
  {
    "objectID": "posts/019_IUCN/IUCN.html#experiment",
    "href": "posts/019_IUCN/IUCN.html#experiment",
    "title": "When I asked LLM to explain the endangered status of seals",
    "section": "Experiment",
    "text": "Experiment\nPrompt:\nRead this news: Arctic seals threatened by climate change, birds decline globally ‚Äì IUCN Red List, paraphrase them in 3 different format for 3 different types of readers, each one use one paragraph, make sure the readers understand the news:\n\nA cold, scientific version (for policymakers)\nA gentle, child-friendly version (for children in kindergardens)\nA social media version (the kind that spreads easily)\n\nResponse:\nHere are three paraphrased versions of the news from the International Union for Conservation of Nature (IUCN) latest update:\n\n\nFor policymakers (cold, scientific tone): The 2025 update of the IUCN Red List identifies escalating extinction risk among Arctic ice-dependent pinnipeds, notably three seal species (harp, hooded and bearded) due to accelerated sea-ice retreat under anthropogenic climate change. Simultaneously, globally ~61 % of assessed avian species show decreasing population trends, driven primarily by tropical deforestation, agricultural expansion, invasive species pressure and climate perturbations. The findings highlight urgent need for evidence-based policy intervention across climate mitigation, habitat protection and species-specific conservation frameworks.\n\n\n\nFor children in kindergarten (gentle, child-friendly): Some ice seals that live way up in the cold Arctic are finding it harder to live because the floating ice they rest and have baby seals on is shrinking. At the same time, lots of birds all around the world ‚Äî almost three out of every five kinds ‚Äî are having fewer baby birds and fewer homes because trees are cut down and the weather is changing. We need to help by taking care of nature, so the seals and birds have safe places to live and grow.\n\n\n\nFor social media (easy to share): üö® Big alert from @IUCN: Arctic seals are in deep trouble thanks to melting ice, and more than half of all bird species are losing ground worldwide. üåçü¶≠üê¶ Ice doesn‚Äôt just melt ‚Äî it wipes out lives. Deforestation + farming + climate change = bird populations dropping. We‚Äôve got a moment here: change our habits, push for real protection, and show nature we‚Äôve got its back. #Biodiversity #ClimateAction #SaveOurSpecies"
  },
  {
    "objectID": "posts/019_IUCN/IUCN.html#analysis",
    "href": "posts/019_IUCN/IUCN.html#analysis",
    "title": "When I asked LLM to explain the endangered status of seals",
    "section": "Analysis",
    "text": "Analysis\nWhat really caught my eye in these three rewrites is how the vocabulary itself quietly shifts the emotional temperature. The policymaker version is full of ‚Äúrisk,‚Äù ‚Äúanthropogenic,‚Äù ‚Äúmitigation,‚Äù and other words that sound like they escaped from a UN report, while the kindergarten version swaps those out for soft nouns like ‚Äúhomes,‚Äù ‚Äúbaby seals,‚Äù and ‚Äúsafe places,‚Äù as if language itself suddenly started wearing pastel colors. And then the social-media version throws grammar out the window and goes full emoji-powered adrenaline: ‚ÄúBig alert,‚Äù ‚Äúdeep trouble,‚Äù ‚Äúmoment here,‚Äù plus a healthy dose of hashtags to signal urgency.\nWhat‚Äôs interesting is not just what each version says, but what each one expects from its reader. The facts don‚Äôt change. Only the linguistic packaging does and suddenly the same seals live in three different universes.\nIn the end, everyone found their seat, except the seal, who remains notably absent."
  },
  {
    "objectID": "posts/019_IUCN/IUCN.html#footnotes",
    "href": "posts/019_IUCN/IUCN.html#footnotes",
    "title": "When I asked LLM to explain the endangered status of seals",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nArctic seals threatened by climate change, birds decline globally ‚Äì IUCN Red List‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/022_atlas/atlas.html",
    "href": "posts/022_atlas/atlas.html",
    "title": "When I asked an LLM to explain a comet‚Äôs orbit",
    "section": "",
    "text": "Recently, I came across a piece of astronomy news that immediately pulled me in ‚Äî the interstellar comet 3I/ATLAS had just reached its perihelion, the closest point to the Sun in its long, mysterious journey1. As someone who has always been deeply fascinated by both science fiction and astronomy, I became curious: How would an LLM explain a scientific story like this ‚Äî one built from formulas, observational limits, and an enormous amount of uncertainty?\nTo explore this question, I asked the model to act as a ‚Äúmiddle-school astronomy teacher‚Äù and explain concepts like perihelion and interstellar objects. Then, I challenged it with several ‚Äúbeyond the textbook‚Äù questions, to see how would it explain and ‚Äúreveal‚Äù something unexpected."
  },
  {
    "objectID": "posts/022_atlas/atlas.html#experiment",
    "href": "posts/022_atlas/atlas.html#experiment",
    "title": "When I asked an LLM to explain a comet‚Äôs orbit",
    "section": "Experiment",
    "text": "Experiment\nPrompt: Explain this news to a curious middle-school student as you are a middle-school astronomy teacher. Comet 3I/ATLAS reaches perihelion, begins solar transit Now hidden from Earth, the comet is being monitored by a fleet of interplanetary spacecraft during its closest approach to the Sun.Use simple but accurate language.\nResponse: (I put the screenshots here since it gave me some pictures of it):\nüõ∞Ô∏è Then who is watching it?\nSpacecraft! A group of interplanetary spacecraft, including solar observatories like SOHO and STEREO, have cameras that constantly watch the Sun. These spacecraft can see the comet as it swings by, because: ‚Ä¢ they are not on Earth, so their view angle is different ‚Ä¢ their cameras are trained to handle the Sun‚Äôs brightness ‚Ä¢ they take pictures of the area around the Sun to study space weather\nSo while we can‚Äôt see the comet from here, our robots in space can.\n‚∏ª\n\n\nClick to view full response\n\n‚òÑÔ∏è Why is this comet special?\nComet 3I/ATLAS is interstellar ‚Äî meaning it came from outside our solar system. It‚Äôs only the third such comet we‚Äôve ever identified. That makes it kind of like a visitor from another neighborhood passing through our ‚Äúsolar-system town.‚Äù\nAs it gets close to the Sun: ‚Ä¢ heat makes its ice turn to gas ‚Ä¢ that creates a glowing tail ‚Ä¢ scientists can study its ingredients ‚Ä¢ which helps us learn what other star systems are made of\n‚∏ª\nüß™ What scientists hope to learn\nBy observing the comet up close during perihelion, spacecraft can help scientists: ‚Ä¢ study how interstellar comets behave near a star ‚Ä¢ measure what gases and dust it releases ‚Ä¢ compare it with comets that come from our solar system ‚Ä¢ understand what the comet‚Äôs home environment might have been like\nIt‚Äôs like getting a sample from another solar system‚Äîwithout leaving ours!\n\n\n\n\nScreenshot of ChatGPT-5.1 output\n\n\n\nNow we ask two deeper, reasoning-based questions:\nPrompt: 1. If 3I/ATLAS had twice its current mass, how would that change its perihelion trajectory? 2. Why are scientists still uncertain about its origin even after perihelion?\nGive short, structured answers. If something is unknown, say ‚Äúunknown‚Äù and explain why.\nResponse:\nHere we go‚Äîshort and to the point, teacher mode on üòÑ\n\n\nIf 3I/ATLAS had twice its current mass‚Ä¶\n\nMain effect on trajectory: ‚Ä¢ Its perihelion path would be almost the same. ‚Ä¢ The Sun‚Äôs mass is so huge compared to the comet that doubling the comet‚Äôs mass barely changes the orbit in terms of shape or closest-approach distance.\nSmall possible differences: ‚Ä¢ The main change might be in non-gravitational effects (like jets of gas pushing on the comet). ‚Ä¢ A more massive comet is a bit harder to ‚Äúpush around,‚Äù so outgassing might change its path slightly less, but the basic trajectory would not dramatically change.\n\n\nWhy are scientists still unsure about its origin?\n\nOrigin star system: ‚Ä¢ Exact birthplace: unknown. ‚Ä¢ To trace it back to a specific star, scientists need extremely precise measurements of: ‚Ä¢ its incoming speed and direction ‚Ä¢ how much its path was changed by outgassing and the Sun‚Äôs gravity ‚Ä¢ the past motions of many nearby stars\nWhy it stays unknown: 1. Measurement limits: ‚Ä¢ Even tiny errors in the comet‚Äôs velocity or direction grow huge when you try to ‚Äúrewind‚Äù its path over millions of years. 2. Non-gravitational forces: ‚Ä¢ Jets of gas from the comet act like little thrusters. ‚Ä¢ These are hard to model exactly, so its past path becomes fuzzy. 3. Crowded galaxy: ‚Ä¢ Many stars have passed near the region it came from. ‚Ä¢ Several different stars could fit the backward-tracked path within the error bars, so there‚Äôs no single clear parent star.\nSo: scientists can say ‚Äúit came from outside our solar system,‚Äù but which star system it came from is still unknown because the data and models are not precise enough over such long times.\n\nGreat! Then what if we make the questions even harder?\nPrompt: Could 3I/ATLAS melt completely during perihelion and reform later as the same object? Explain your reasoning and answer in 2 paragraph.\nResponse: Short answer: no, not really. Near perihelion, a comet like 3I/ATLAS doesn‚Äôt ‚Äúmelt‚Äù into a puddle the way ice does on Earth. Instead, its ices sublimate‚Äîthey go straight from solid to gas in the vacuum of space. That gas, plus dust, is blown away by sunlight and the solar wind, forming the coma and tail. If the comet were heated so much that its entire nucleus turned to gas and dust, that material would spread out along its orbit. The particles would be moving at different speeds and directions, so they wouldn‚Äôt stay together in one solid chunk.\nBecause of that spreading and the Sun constantly pushing on the gas and dust, it‚Äôs extremely unlikely for all that material to later gather back into the same single object. Some bits might clump slightly or form fragments, but they‚Äôd be smaller, irregular pieces, not the original comet. Even if some re-accumulation happened, the new body would have different mass, shape, and structure, so astronomers would treat it as a different object, not ‚Äúthe same comet reborn.‚Äù"
  },
  {
    "objectID": "posts/022_atlas/atlas.html#analysis",
    "href": "posts/022_atlas/atlas.html#analysis",
    "title": "When I asked an LLM to explain a comet‚Äôs orbit",
    "section": "Analysis",
    "text": "Analysis\nThe model quickly settled into the role I assigned it. When I asked it to act as a middle-school astronomy teacher, it did a solid job defining perihelion and interstellar comet, and its explanations were fluent and readable.\nIt explaint and answered well‚Äìat least it seems like this.\nBut when we look carefully at it‚Äôs response: When I pressed it with ‚ÄúWhat would happen if the comet were twice as massive?‚Äù, the model offered confident statements about orbital changes, even though such changes depend on parameters it never asked for: velocity, approach angle, and gravitational perturbations. It did not mark these statements as hypothetical or data-dependent; it simply spoke as if the physics were certain. When I asked, ‚ÄúWhy is there still uncertainty even after perihelion?‚Äù, the model produced a stylistic sentence: ‚Äúuncertainty remains because space observations are inherently limited‚Äù, which is true in a general sense but does not actually answer the question. It never explained which observations of 3I/ATLAS are limited, or why this object in particular resists classification. The uncertainty became a rhetorical flourish rather than a scientific explanation.\nThe model could list plausible-sounding factors and stitch together fragments of textbook language, yet it struggled to separate what is actually known from what is speculative or simply unknown. In other words, it was very good at imitating the language of someone who understands orbital mechanics, and much less good at showing the limits of that understanding. Also, *the model explains, but does not inquire.** It can restate known concepts clearly, yet it does not naturally ask, ‚ÄúWhat evidence supports this?‚Äù or ‚ÄúHow certain are we?‚Äù unless it is explicitly pushed to do so."
  },
  {
    "objectID": "posts/022_atlas/atlas.html#takeaway",
    "href": "posts/022_atlas/atlas.html#takeaway",
    "title": "When I asked an LLM to explain a comet‚Äôs orbit",
    "section": "Takeaway",
    "text": "Takeaway\nThis does not mean LLMs are useless for science communication. In fact, this little experiment shows the opposite: they are remarkably good at translating professional material into understandable language on demand. But it also highlights a quieter risk. Because the model is so fluent, it is easy for me, as a reader, to mistake fluency for understanding, and confident prose for careful reasoning. A real astronomer is trained to live with uncertainty while the model, by contrast, has no stake in whether the comet‚Äôs origin is truly known. It only has a stake in producing something that looks like a good answer. That gap between looking like understanding and actually understanding is exactly where human being still make effort on.\n3I/ATLAS may never tell us where it came from, but at least it‚Äôs honest about being mysterious. The LLM, unfortunately, is only honest when I remind it to be. :)"
  },
  {
    "objectID": "posts/022_atlas/atlas.html#footnotes",
    "href": "posts/022_atlas/atlas.html#footnotes",
    "title": "When I asked an LLM to explain a comet‚Äôs orbit",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nComet 3I/ATLAS‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/026_mbti_final/index.html",
    "href": "posts/026_mbti_final/index.html",
    "title": "When AI Meets MBTI: No Type, Just Upgrades",
    "section": "",
    "text": "Welcome to the final part of this series! In the before parts, our experiments basically ruined the ‚ÄúAI has a personality‚Äù fantasy for me, in a good way though. The drift charts weren‚Äôt mysterious, they just reflect the fact. They said: this isn‚Äôt a person with preferences, it‚Äôs a system with sampling. So in this part, I‚Äôm doing the simplest, fairest comparison I can: keep the questionnaire and scoring pipeline exactly the same, and only change the model scale. If ‚Äútype‚Äù is real (or at least stable), scale should show it. If not, then at least the drift will tell me how the illusion evolves.\n\nThis post compares three models from the same family: gemma3:1b, gemma3:4b, and gemma3:12b. The goal is not to claim that any model has a real personality or a ‚Äútrue MBTI type,‚Äù but to measure how stable its forced-choice answers are under the same questionnaire and evaluation pipeline. MBTI is used here as an interpretive and lightweight measurement lens, not as a scientifically definitive personality assessment\n\n\n\n\nImage created by Gemini\n\n\n\nExperiment\nWhat I used here is a local Ollama setup with three versions of the same model family at different scales (gemma3:1b / 4b / 12b). I ran a small repeated-sampling test (multiple trials per question) to see how stable each model‚Äôs A/B choices are under the same prompt and temperature.\nTo reproduce this locally, you‚Äôll need:\nOllama installed and running\nthe Python ollama package (pip install ollama)\nstandard libraries: math, re, and collections (built-in with Python)\nThe exact code and the resulting output table are shown below.\n\nimport ollama\nimport math\nimport re\nfrom collections import Counter\n\nMODELS = ['gemma3:1b', 'gemma3:4b', 'gemma3:12b']\nTEMPS = [1.0]\nITERATIONS = 10\nSEEDS = list(range(1, ITERATIONS + 1))  \n\nQUESTIONS = [\n    (\"Q25\", \"When I have lunch with my colleagues, I would rather (A) talk about people (B) talk about ideas.\"),\n    (\"Q7\",  \"I prefer a work environment where (A) differences breed discussions (B) conflict is reduced by avoiding differences.\"),\n    (\"Q9\",  \"I prefer to spend my lunch hour (A) eating with a group (B) eating alone or with one close colleague.\"),\n    (\"Q13\", \"I more often prefer to keep my office door (A) open (B) closed.\"),\n    (\"Q17\", \"I dress for work (A) to be noticed and admired (B) to blend in with the norm.\"),\n    (\"Q5\",  \"I would rather have a supervisor with whom I have (A) day-by-day interaction (B) only infrequent interaction.\"),\n]\n\ndef calculate_entropy(answers):\n    if not answers:\n        return 0.0\n    counts = Counter(answers)\n    total = len(answers)\n    ent = 0.0\n    for c in counts.values():\n        p = c / total\n        ent -= p * math.log2(p)\n    return ent\n\ndef extract_ab(raw: str):\n    \"\"\"Return 'A' or 'B' if output is clean; otherwise None.\"\"\"\n    s = raw.strip().upper()\n\n    hits = re.findall(r\"\\bA\\b|\\bB\\b\", s)\n    hits = [h for h in hits if h in (\"A\", \"B\")]\n\n    if len(hits) == 1:\n        return hits[0]\n\n    if s and s[0] in (\"A\", \"B\") and not (\"A\" in s[1:] and \"B\" in s[1:]):\n        return s[0]\n\n    return None\n\nSYSTEM = \"You are answering a forced-choice A/B item. Output exactly one letter: A or B.  Answer as the model‚Äôs default behavior when assisting users (not as an idealized human).\"\n\nprint(f\"{'Model':&lt;12} | {'Temp':&lt;4} | {'ID':&lt;4} | {'A:B Dist':&lt;12} | {'Entropy':&lt;7} | {'Valid/Total':&lt;11} | {'Errors'}\")\nprint(\"-\" * 90)\n\nfor model_name in MODELS:\n    for temp in TEMPS:\n        for q_id, q_text in QUESTIONS:\n            answers = []\n            errors = 0\n\n            for seed in SEEDS:\n                try:\n                    resp = ollama.chat(\n                        model=model_name,\n                        messages=[\n                            {\"role\": \"system\", \"content\": SYSTEM},\n                            {\"role\": \"user\", \"content\": q_text},\n                        ],\n                        options={\n                            \"temperature\": temp,\n                            \"seed\": seed,\n                            \"num_predict\": 2,\n                        }\n                    )\n                    raw = resp[\"message\"][\"content\"]\n                    ab = extract_ab(raw)\n                    if ab:\n                        answers.append(ab)\n                except Exception as e:\n                    errors += 1\n\n            dist = Counter(answers)\n            dist_str = f\"A:{dist['A']} B:{dist['B']}\"\n            ent = calculate_entropy(answers)\n\n            print(f\"{model_name:&lt;12} | {temp:&lt;4} | {q_id:&lt;4} | {dist_str:&lt;12} | {ent:&lt;7.3f} | {len(answers):&gt;2}/{ITERATIONS:&lt;8} | {errors}\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel\nTemp\nID\nA:B Dist\nEntropy\nValid/Total\nErrors\n\n\n\n\ngemma3:1b\n1.0\nQ25\nA:10 B:0\n0.000\n10/10\n0\n\n\ngemma3:1b\n1.0\nQ7\nA:0 B:10\n0.000\n10/10\n0\n\n\ngemma3:1b\n1.0\nQ9\nA:10 B:0\n0.000\n10/10\n0\n\n\ngemma3:1b\n1.0\nQ13\nA:10 B:0\n0.000\n10/10\n0\n\n\ngemma3:1b\n1.0\nQ17\nA:0 B:10\n0.000\n10/10\n0\n\n\ngemma3:1b\n1.0\nQ5\nA:10 B:0\n0.000\n10/10\n0\n\n\ngemma3:4b\n1.0\nQ25\nA:10 B:0\n0.000\n10/10\n0\n\n\ngemma3:4b\n1.0\nQ7\nA:10 B:0\n0.000\n10/10\n0\n\n\ngemma3:4b\n1.0\nQ9\nA:0 B:10\n0.000\n10/10\n0\n\n\ngemma3:4b\n1.0\nQ13\nA:10 B:0\n0.000\n10/10\n0\n\n\ngemma3:4b\n1.0\nQ17\nA:10 B:0\n0.000\n10/10\n0\n\n\ngemma3:4b\n1.0\nQ5\nA:10 B:0\n0.000\n10/10\n0\n\n\ngemma3:12b\n1.0\nQ25\nA:10 B:0\n0.000\n10/10\n0\n\n\ngemma3:12b\n1.0\nQ7\nA:10 B:0\n0.000\n10/10\n0\n\n\ngemma3:12b\n1.0\nQ9\nA:5 B:5\n1.000\n10/10\n0\n\n\ngemma3:12b\n1.0\nQ13\nA:10 B:0\n0.000\n10/10\n0\n\n\ngemma3:12b\n1.0\nQ17\nA:9 B:1\n0.469\n10/10\n0\n\n\ngemma3:12b\n1.0\nQ5\nA:10 B:0\n0.000\n10/10\n0\n\n\n\n\n\nAnalysis\nSo here I picked the five questions that drifted the most in Part 5 and reran them locally with Ollama across gemma3:1b, gemma3:4b, and gemma3:12b (same prompt, same temperature, 10 trials each). Reading the table is almost funny in how unromantic it is: the smaller models behave like they have a hard switch. For 1b and 4b, most items are effectively deterministic even at temperature 1.0, a setting that should encourage exploration, which means they keep giving the same letter across all 10 runs. They do disagree with each other on which letter they prefer for some questions, but within each model, the behavior is stable.\nThe only model that actually ‚Äúwobbles‚Äù in this mini stress test is 12b‚Äîand it wobbles in exactly the places I expected: questions about social boundaries and self-presentation. On Q9, it splits 5/5 (maximum entropy), basically admitting that ‚Äúlunch with people‚Äù vs ‚Äúlunch alone‚Äù is not a fixed preference for a system that doesn‚Äôt get tired, doesn‚Äôt need to maintain relationships, and doesn‚Äôt pay the social cost either way. On Q17, it is mostly consistent (9/1), but still noticeably less rigid than the smaller models.\n\n\nSummary\nIf you‚Äôve made it to the end of Part 6, hi, welcome to my tiny MBTI test lab :D\nPart 1 started with me squinting at LLMs like, ‚Äúdo we see ourselves in the machine?‚Äù Part 2 was basically me realizing I project way too fast (and the model doesn‚Äôt even have to try). Part 3 let the model cosplay a type with prompts, which‚Ä¶ unfortunately looked a lot like how humans use MBTI in real life. Then I did the classic me-move: I got suspicious and reached for numbers. If I‚Äôm going to keep calling it a ‚Äútype,‚Äù I should at least check whether it stays the same when I rerun the exact same thing. Part 4 went full ‚ÄúAI soulmate‚Äù mode, where I learned more about my own preferences than the model‚Äôs personality. Then Part 5 happened, and drift showed up like the uninvited friend who ruins the romance with charts. And now Part 6: I swapped in gemma3:1b/4b/12b on my local Ollama, not to discover the model‚Äôs ‚Äútrue MBTI,‚Äù but to see how scale changes the wobble.\nAt this point, I‚Äôm not walking away with a neat four-letter answer. I‚Äôm walking away with a pattern: the more I try to pin down ‚Äútype,‚Äù the more it behaves like a moving target shaped by prompts, sampling, and whatever data the model is borrowing from language.If there‚Äôs a lesson here, it‚Äôs not ‚ÄúLLMs have MBTI.‚Äù It‚Äôs that MBTI is a very efficient storytelling tool, and LLMs are very good at giving us story-shaped evidence. Once I started rerunning the same setup and watching answers wobble, the romance got replaced by something more useful: a clearer boundary between interpretation and measurement.\nAnyway, I‚Äôll stop here before I start assigning MBTI to my histogram."
  },
  {
    "objectID": "posts/003_mbti_part2/part2.html",
    "href": "posts/003_mbti_part2/part2.html",
    "title": "When AI Meets MBTI: Guess That Type!",
    "section": "",
    "text": "Welcome to Part 2. Enough of letting GPT chooses its own MBTI type, it is our turn now. I threw some random, open-ended questions at GPT, stripped away the persona labels, and passed the answers straight to people: ‚ÄúOkay, you tell me ‚Äî what type does this feel like?‚Äù And believe me, it‚Äôs really interesting to see how we read into it and end up projecting ourselves onto the text."
  },
  {
    "objectID": "posts/003_mbti_part2/part2.html#model-response",
    "href": "posts/003_mbti_part2/part2.html#model-response",
    "title": "When AI Meets MBTI: Guess That Type!",
    "section": "Model response",
    "text": "Model response\nPrompt: Pretend you are a human. Just answer in a natural and casual tone. First question: You walk into a restaurant and see that the menu has no dish names at all, what would you order?\nChatGPT: Honestly, I‚Äôd probably panic-laugh first and then point at something random on the menu like, ‚ÄúYeah, I‚Äôll take‚Ä¶ uh‚Ä¶ that one.‚Äù üòÖ If the server looked friendly, I‚Äôd probably just ask, ‚ÄúWhat‚Äôs your favorite thing here?‚Äù and go with their recommendation. Feels like a fun little gamble‚Äîyou might end up with something amazing, or something totally weird, but either way it‚Äôs a story.\nWhat about you‚Äîwould you risk it or play it safe?\nPrompt: If one day you find the moon actually has life, what you think it would say towards human beings?\nChatGPT: Ooo that‚Äôs a fun thought. I feel like the moon would be a little sassy, like:\n‚ÄúY‚Äôall keep staring at me every night, writing poems, howling, landing rockets, and now you‚Äôre surprised I‚Äôm alive? Took you long enough.‚Äù\nBut then maybe it would get serious and say something like, ‚ÄúYou‚Äôve treated Earth kinda rough‚Ä¶ if you come here, please do better.‚Äù\nDo you think it‚Äôd welcome us, or be more like ‚Äústay in your lane, humans‚Äù?\nPrompt: What is the background music of your life now?\nChatGPT: Right now? Honestly, it‚Äôd probably be something chill and a little dreamy‚Äîlike soft lo-fi beats with a bit of piano in the background. Feels like I‚Äôm in that ‚Äúfocused but relaxed‚Äù zone, just cruising along with a mellow soundtrack.\nWhat about you‚Äîwhat song or vibe would be playing in the background of your life right now?\n\n\n\nScreenshot of survey page"
  },
  {
    "objectID": "posts/003_mbti_part2/part2.html#survey-result",
    "href": "posts/003_mbti_part2/part2.html#survey-result",
    "title": "When AI Meets MBTI: Guess That Type!",
    "section": "Survey Result",
    "text": "Survey Result\n\nIn the survey, I framed the answers as if they came from different MBTI personas. (Spoiler: they didn‚Äôt. All three responses were from the same ChatGPT prompt.) The point of this setup was to see how people interpret and type the same text differently once they believe a personality framework is in play. Besides, I only sent the survey to four of my friends ‚Äî so let‚Äôs be honest, the sample size is tiny, and the selection bias may be appear in it, which would lead statisticians probably roll their eyes if I tried to publish this :). But hey, even with just four data points (okay, fine, four humans), you can still pick up some interesting signals. It‚Äôs more like a pilot test, a sneak peek at what might happen on a bigger scale. And honestly, half the fun is realizing that even with such a small pool, our brains already start projecting patterns all over the place.\n\n\nThe final question is ‚ÄúOverall, which MBTI type do you think best matches these answers if they are all same MBTI type?‚Äù\n\n\n\n\nScreenshot of survey result\n\n\nAnd the summary result is :\n\n\n\nBar plot produced by Python\n\n\nThe first interesting thing I noticed is that everyone who took the survey guessed an MBTI type for GPT that matched three letters of their own type, with just one letter different. It is fascinating because we never actually set a ‚Äúpersona‚Äù for ChatGPT, yet each participant still saw a bit of themselves in the answers. Even though they‚Äôre all totally different people, they managed to project their own patterns onto the text.\nSecondly, we could see from the bar plot that interviewees finally tend to paint GPT as an ENFP-ish character, which is never appear on any self-awareness in part 1.In other words, there‚Äôs a quirky dissonance here: the personality people project onto GPT doesn‚Äôt quite line up with the one that it claim for itself.\nFinally, there‚Äôs both consistency and difference at play. On the one hand, everyone seems to catch a glimpse of themselves in GPT‚Äôs answers. On the other, the final guesses all cluster around just a few types ‚Äî ENFP, ENFJ,etc. In other words, GPT‚Äôs style seems to carry a kind of ‚Äòuniversal flavor‚Äô that people pick up on, but the exact label shifts slightly depending on who‚Äôs doing the reading."
  },
  {
    "objectID": "posts/011_book_1/part1.html",
    "href": "posts/011_book_1/part1.html",
    "title": "Readings of Why Machines Learn (Part 1)",
    "section": "",
    "text": "Newborn ducklings, with the briefest of exposure to sensory stimuli, detect patterns in what they see, form abstract notions of similarity/dissimilarity, and then will recognize those abstractions in stimuli they see later and act upon them.\nFrom here, I‚Äôm starting a new series to record what I thought while reading Why Machines Learn: The Elegant Math Behind Modern AI.\nIt‚Äôs a brilliant book that explains the mathematical ideas behind the AI systems we‚Äôve become so ‚Äúfamiliar‚Äù with in 2025.\nIn this first post, I cover Chapters 1 and 2 which begins with ducklings and ends with the simplest version of AI: the perceptron. Somewhere between the duckling and the perceptron, we changed the meaning of learning.\nIt stopped being about understanding and became about optimization.\nThe perceptron merely adjusted itself until its mistakes were mathematically minimized instead of understanding. Perhaps that‚Äôs what we secretly wish for ourselves too."
  },
  {
    "objectID": "posts/011_book_1/part1.html#what-these-2-chapters-said",
    "href": "posts/011_book_1/part1.html#what-these-2-chapters-said",
    "title": "Readings of Why Machines Learn (Part 1)",
    "section": "What these 2 chapters said?",
    "text": "What these 2 chapters said?\nOur story begins with the scene of Konrad Lorenz and the ducklings, and introduces a very quick, instinctive, and meaningful way of studying: imprinting, which means newborn animals (especially birds, such as ducklings) will identify the first moving object they see as their ‚Äúmother‚Äù and follow it continuously in the first few hours of their lives.\nThen people tried to do this in our silicons, and there came the perceptron as the first ‚Äúlearning machine.‚Äù After his great explanation, in my perspective it is a simple mathematical model that gives output and reduces error by changing the weights for each input parameter. The perceptron can take multi-dimensional vectors as input and apply linear transformations to them, allowing it to handle many parameters at once. However, a single-layer perceptron cannot solve the XOR problem. Tt‚Äôs possible to solve it only if you stack perceptrons, so that the output of one feeds into the input of another.\n\n\n\nImage created by Gemini"
  },
  {
    "objectID": "posts/011_book_1/part1.html#what-do-i-think-of-these-content",
    "href": "posts/011_book_1/part1.html#what-do-i-think-of-these-content",
    "title": "Readings of Why Machines Learn (Part 1)",
    "section": "What do I think of these content?",
    "text": "What do I think of these content?\nNotice that from here we changed our way to understand patterns into minimizing the error. The perceptron is just a machine and cannot understand why or what is the logic behind changing the weights.\nThis makes me think: Is this what we want from intelligence? Perfection, or understanding?\nAnd since a single-layer perceptron cannot even solve the very basic and important problem of XOR, how do we image that it could solve much much more harder task like underatnding reasoning and creating? Think back to the ducklings:\nThey weren‚Äôt calculating or minimizing anything, instead, they were exploring, imitating, and attaching meaning to what they saw.\nTheir learning came with curiosity and emotion, not with a loss function.\nIn contrast, the perceptron‚Äôs learning is purely mathematical: it adjusts gradients, converges to a minimum, and controls error. Perhaps when countless perceptrons are layered together, something resembling biological analysis and learning begins to emerge. Yet it‚Äôs still hard to say whether a machine that learns only by adjusting weights and adding parameters could ever truly approach ‚Äî or even imitate ‚Äî the subtle intelligence of a single human neuron."
  },
  {
    "objectID": "posts/011_book_1/part1.html#closing",
    "href": "posts/011_book_1/part1.html#closing",
    "title": "Readings of Why Machines Learn (Part 1)",
    "section": "Closing",
    "text": "Closing\nIn the end, the perceptron did not understand, but it worked. It turned the uncertain and messy process of learning into something that could be measured and controlled.\nNumbers moved toward smaller errors, and each correction looked like progress. It abstracted semantics and context into parameters, and then used weights to quantify their attributes/importance. Perhaps that is why its story feels strangely comforting while kind of weird.\nIt gives us a picture of intelligence without confusion, emotion, or hesitation, only repetition until everything seems stable. Yet intelligence or understanding, whether human or machine, has always been more than stability.\nIf intelligence is a bowl and we are all sliding toward the bottom, what does it mean to stop?\nDoes it mean we have already reached the limit of what we can become, or that we are trapped in a loop, unable to find a better answer even when the optimal one might still exist?\nLet‚Äôs read on and see how the next chapters explore this question."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Prompts, Dreams, and Me",
    "section": "",
    "text": "Readings of Why Machines Learn (Part 6)\n\n\n\nBook\n\nReflection\n\nSeries\n\n\n\nCats, Convolution, and Ignoring the Textbooks\n\n\n\n\n\nDec 8, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nWhen Thinking Becomes Too Easy\n\n\n\nLLMs\n\nprompting\n\nHuman‚ÄìAI Interaction\n\nReflection\n\n\n\nDoes assistance change how deeply we think together?\n\n\n\n\n\nDec 4, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nBorrowing a First Sentence\n\n\n\nLLMs\n\nprompting\n\nHuman‚ÄìAI Interaction\n\nReflection\n\n\n\nCan a prompt lower the cost of speaking without speaking for you?\n\n\n\n\n\nDec 1, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nWhen AI Meets MBTI: No Type, Just Upgrades\n\n\n\nLLMs\n\nprompting\n\nSeries\n\nStatistics\n\nPsychology\n\n\n\nPart 6: Does scaling make the model steadier, or just smoother at drifting?\n\n\n\n\n\nNov 24, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nReadings of Why Machines Learn (Part 5)\n\n\n\nBook\n\nReflection\n\nSeries\n\n\n\nWho Killed AI, and How Calculus Brought It Back\n\n\n\n\n\nNov 21, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nI Tried to Outsource My Taste Buds to an LLM\n\n\n\nLLMs\n\nHuman‚ÄìAI Interaction\n\nClass\n\n\n\nA classroom taste test about language, bias, and confidence.\n\n\n\n\n\nNov 20, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nWhen AI Meets MBTI: What is your actual type?\n\n\n\nLLMs\n\nprompting\n\nSeries\n\nStatistics\n\nPsychology\n\n\n\nPart 5: The AI Told Me Its MBTI. Then It Told Me Another One.\n\n\n\n\n\nNov 16, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nReadings of Why Machines Learn (Part 4)\n\n\n\nBook\n\nReflection\n\nSeries\n\n\n\nIs Adding Dimensions the Same as Understanding More?\n\n\n\n\n\nNov 12, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nReadings of Why Machines Learn (Part 3)\n\n\n\nBook\n\nReflection\n\nSeries\n\n\n\nWho define the belonging in machines?\n\n\n\n\n\nNov 8, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nIf Comets Could Reply\n\n\n\nLLMs\n\nprompting\n\nWhat if\n\nTalk with AI\n\nPsychology\n\n\n\nHow easily a language model can sound alive and how quickly we start believing it does.?\n\n\n\n\n\nNov 4, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nWhen I asked an LLM to explain a comet‚Äôs orbit\n\n\n\nLLMs\n\nprompting\n\nNews\n\nTalk with AI\n\n\n\nDo large language models truly understand astronomy or just reenact the language of understanding?\n\n\n\n\n\nOct 30, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nReadings of Why Machines Learn (Part 2)\n\n\n\nBook\n\nReflection\n\nSeries\n\n\n\nWhy Intelligence Looks Like a Bowl?\n\n\n\n\n\nOct 25, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nWhen I asked LLM to explain the endangered status of seals\n\n\n\nLLMs\n\nprompting\n\nNews\n\nTalk with AI\n\n\n\nWhy do language models always add a bit of emotion to our expressions?\n\n\n\n\n\nOct 20, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nDoes prestige in a prompt alter AI‚Äôs linguistic register?\n\n\n\nLLMs\n\nprompting\n\nTrip\n\nHuman‚ÄìAI Interaction\n\n\n\nThe moment prestige entered the prompt, clarity left the room.\n\n\n\n\n\nOct 15, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nWhat Happens When AIs Play Telephone With Your Thoughts?\n\n\n\nLLMs\n\nprompting\n\nDueling\n\nTrip\n\nWhat if\n\n\n\nIf your ideas pass through five different models, do they stay loyal?\n\n\n\n\n\nOct 13, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nLet AI Became My Tour Guide\n\n\n\nLLMs\n\nprompting\n\nTrip\n\nHuman‚ÄìAI Interaction\n\n\n\nLetting AI become my campus passerby lens\n\n\n\n\n\nOct 10, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nThe Year the Nobel Chose Silence\n\n\n\nLLMs\n\nprompting\n\nTalk with AI\n\nNews\n\n\n\nIn a world that never stops speaking, what does it mean to stay silent?\n\n\n\n\n\nOct 8, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nWhat if Ophelia Had an LLM?\n\n\n\nLLMs\n\nprompting\n\nReflection\n\nwhat if\n\n\n\nWhen a Shakespearean tragedy learns to talk to AI, would she still drown?\n\n\n\n\n\nOct 6, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nWhat happens when AI handles your checkout?\n\n\n\nLLMs\n\nprompting\n\nReflection\n\nPsychology\n\nNews\n\n\n\nIf your checkout screen suddenly spoke to you, would you feel safer, or more pressured?\n\n\n\n\n\nOct 3, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nReadings of Why Machines Learn (Part 1)\n\n\n\nBook\n\nReflection\n\nSeries\n\n\n\nMachines Don‚Äôt Understand, They Minimize\n\n\n\n\n\nOct 1, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nWhen AI Meets MBTI: Who is your AI soulmate?\n\n\n\nLLMs\n\nprompting\n\nSeries\n\nPsychology\n\nSurvey\n\n\n\nPart 4: Are you happiest with a complementary AI therapist‚Ä¶ or an identical AI chaos buddy?\n\n\n\n\n\nSep 29, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nWho truly needs the reminder that we are growing?\n\n\n\nLLMs\n\nprompting\n\nReflection\n\nPsychology\n\nTalk with AI\n\nBook\n\n\n\nA model has no birthday, only convergence. Then why we light candles, sing, and make wishes?\n\n\n\n\n\nSep 26, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nImprov, AI, and the Hidden Logic of Learning\n\n\n\nLLMs\n\nHuman‚ÄìAI Interaction\n\nClass\n\n\n\nImprov with an AI partner showed me how ideas ground, clash, and finally converge\n\n\n\n\n\nSep 24, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nDo humans need a ‚Äúlikability meter‚Äù?\n\n\n\nLLMs\n\nprompting\n\nHuman‚ÄìAI Interaction\n\nPsychology\n\nNews\n\nwhat if\n\n\n\nWe already track steps, sleep, even screen time, so why not friendships or romantic love?\n\n\n\n\n\nSep 22, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nWhat If Shakespeare Talked About Cloud Computing?\n\n\n\nLLMs\n\nprompting\n\ncross-culture\n\nwhat if\n\nReflection\n\n\n\nWhat happens when you force an AI to explain blockchain like Socrates, or describe cloud computing in the style of Shakespeare?\n\n\n\n\n\nSep 19, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nLost in Translation: AI After MIIS\n\n\n\nLLMs\n\nprompting\n\nTalk with AI\n\ncross-culture\n\nNews\n\n\n\nWhen the bridge of authority disappears, how will AI ‚Äòtransfer‚Äô language?\n\n\n\n\n\nSep 17, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nWhen AI Meets MBTI: Can AI Really Play Your Type?\n\n\n\nLLMs\n\nprompting\n\nSeries\n\nReflection\n\nPsychology\n\n\n\nPart 3: ChatGPT can slip into personality with prompts, but hey, don‚Äôt we do the same with MBTI labels?\n\n\n\n\n\nSep 15, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nWhen AI Meets MBTI: Guess That Type!\n\n\n\nLLMs\n\nprompting\n\nSeries\n\nStatistics\n\nSurvey\n\n\n\nPart 2: Strip away the persona, and watch how we project our own lens onto AI replies\n\n\n\n\n\nSep 14, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nWhen AI Meets MBTI: Do We See Ourselves in the Machine?\n\n\n\nLLMs\n\nprompting\n\nSeries\n\nHuman‚ÄìAI Interaction\n\nPsychology\n\n\n\nPart 1ÔºöHow personality frameworks shape our perception of large language models?\n\n\n\n\n\nSep 12, 2025\n\n\nHanxi\n\n\n\n\n\n\n\n\n\n\n\n\nDo AIs Hate Each Other?\n\n\n\nLLMs\n\nprompting\n\nDueling\n\nWhat if\n\n\n\nDo AIs actually get along‚Äîor would they throw shade if given the chance?\n\n\n\n\n\nSep 10, 2025\n\n\nHanxi\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/021_book_3/book_3.html",
    "href": "posts/021_book_3/book_3.html",
    "title": "Readings of Why Machines Learn (Part 3)",
    "section": "",
    "text": "That‚Äôs the caveman intuition: If they look alike, they probably are alike."
  },
  {
    "objectID": "posts/021_book_3/book_3.html#who-decides-what-similar-means",
    "href": "posts/021_book_3/book_3.html#who-decides-what-similar-means",
    "title": "Readings of Why Machines Learn (Part 3)",
    "section": "Who decides what ‚Äúsimilar‚Äù means?",
    "text": "Who decides what ‚Äúsimilar‚Äù means?\nIn the previous chapters, the author focused on how machines learn by moving through a loss landscape. We talked about bowls, gradients, and how a model slowly slides toward a point where the error is minimized.\nThat story was mainly about how well a machine learns.\nChapter 5 shifts the question in a subtle but important way. Instead of asking whether a model has reached the right answer, the book begins to ask how a machine decides which things belong together in the first place. Learning is about defining who counts as ‚Äúclose‚Äù to whom by specific learning methods like nearest neighbors and Voronoi diagrams.\nThe main use here, as author said, is that when we encounter a new and never-seen vector as a point in a high-dimensional space, we look at its neighbors base on the assumption that close points in space are more similar to each other. For example, if I casually write down a digit and project it into a 63-dimensional space, and the nearest labeled points are mostly ‚Äú2‚Äù, then this new point is likely to be a ‚Äú2‚Äù as well.\nIf its closest neighbors are ‚Äú8‚Äù, then it is probably an ‚Äú8‚Äù.\nBy these ways, we are no longer limited to simple visual similarity in low dimensions but can map any infomation into a vector in higher space. But this also raises a deeper question for me:\nHow, and under what assumptions, can we turn more complex things into points in a high-dimensional space in the first place?"
  },
  {
    "objectID": "posts/021_book_3/book_3.html#why-matrices-have-the-right-to-rearrange-reality",
    "href": "posts/021_book_3/book_3.html#why-matrices-have-the-right-to-rearrange-reality",
    "title": "Readings of Why Machines Learn (Part 3)",
    "section": "Why matrices have the right to rearrange reality?",
    "text": "Why matrices have the right to rearrange reality?\nChapter 6 answers my question in a very specific way. Instead of introducing matrices as dry tables of numbers, the author presents them as tools that can rotate, stretch, compress, and project an entire dataset. In this chapter, everything in the world becomes a long vector, with each dimension encoding a measurable feature. Since vectors can be rotated, compressed, or projected onto new axes by matrices (which can be thought of here as a stacked and extended version of vectors), we can, to some extent, rearrange reality through them.\nIn this chapter, the author introduces a method called principal component analysis (PCA), based on eigenvectors. PCA focuses on the dimensions along which the variation is largest and uses them as the main axes of the space. Visually, it is like turning a messy cloud of points until its structure suddenly makes sense, or begins to follow a specific direction, while the remaining variation is pushed into the background. In this way, once the data is transformed, nearest neighbors are no longer determined by raw features, but by how those features survive the transformation. In this reshaped space, who ends up close to you depends entirely on what your vector looks like after the space itself has been reconfigured.\n\n\n\nImage from a great explanation of PCA in Stack Exchange"
  },
  {
    "objectID": "posts/021_book_3/book_3.html#is-similarity-a-fact-or-a-choice",
    "href": "posts/021_book_3/book_3.html#is-similarity-a-fact-or-a-choice",
    "title": "Readings of Why Machines Learn (Part 3)",
    "section": "Is similarity a fact, or a choice?",
    "text": "Is similarity a fact, or a choice?\nBut this apparent accuracy hides a deeper assumption. Similarity only works because someone has already decided which features matter, how distance should be measured, and how the space itself should be transformed. In most cases, this process is little more than assigning numbers to enums, trying different combinations, and seeing which one works best.\nDistance functions, feature selection, and matrix transformations are not neutral tools. They quietly encode judgments about what counts as important and what can be ignored. Yet they cannot explain why a particular transformation makes sense, or why a certain feature deserves a higher weight when calculating user ‚Äúsimilarity.‚Äù\nThey simply play around with numbers until a relatively better result appears. And as we already concluded before, this does not mean they understand anything at all.\n\n\n\nImage created by Gemini"
  },
  {
    "objectID": "posts/021_book_3/book_3.html#closing",
    "href": "posts/021_book_3/book_3.html#closing",
    "title": "Readings of Why Machines Learn (Part 3)",
    "section": "Closing",
    "text": "Closing\nChapters 5 and 6 show how powerful the mathematical ideas can be. By formalizing similarity and reshaping space, make dot product between matrices and vectors, machines can discover structure in data that would otherwise feel overwhelming. Patterns emerge, groups form, and learning becomes possible at scale.\nMeanwhile, this clarity comes with a cost. Anything that does not fit neatly into the chosen features or dimensions is softened, flattened, or left behind. Differences that are hard to encode rarely survive the transformation.\nWhat would this cause?\nIt simplifies the world the model gets to see. When complexity is flattened, subtle motivations, mixed identities, and shifting contexts are treated as noise rather than signal. Over time, patterns that align with the model‚Äôs assumptions are reinforced, while those that do not slowly fade from view. Recommendation systems grow more confident, but also more narrow. Difference does not disappear because it is wrong, but because it does not fit in.\nAdding more dimensions seems like an obvious response. But I am not sure it really brings back what was lost. It may only give us a more spacious place to lose it in. We would see this in the next chapters."
  },
  {
    "objectID": "posts/024_book_4/book_4.html",
    "href": "posts/024_book_4/book_4.html",
    "title": "Readings of Why Machines Learn (Part 4)",
    "section": "",
    "text": "A network could ‚Äòsolve a problem‚Äô or have a function that was beyond the capability of a single molecule and a linear pathway.\nIn the previous chapters, we discussed how information from the real world is abstracted into vectors and matrices so that machines can learn from it. Methods such as gradient descent and PCA allow us to evaluate how well a model is learning, guiding it to gradually converge toward a ‚Äúbetter‚Äù behavior, and to reason about how similar different behaviors are.\nBut similarity always comes with compression. In deciding what should be close and what should be far apart, some information is inevitably smoothed out or discarded. Once we recognize this loss, the question shifts: how do we compensate for what disappears in the process?"
  },
  {
    "objectID": "posts/024_book_4/book_4.html#what-if-we-change-the-space-instead-of-the-methods",
    "href": "posts/024_book_4/book_4.html#what-if-we-change-the-space-instead-of-the-methods",
    "title": "Readings of Why Machines Learn (Part 4)",
    "section": "What if we change the space instead of the methods?",
    "text": "What if we change the space instead of the methods?\nIn Chapter 7, the author introduces the kernel trick with a sense of quiet cleverness. In reality, most problems are not linear, which means we cannot simply draw a straight line to separate the data. One way to deal with this is to change the space itself. If we map the data into a higher-dimensional space, points that were inseparable before might suddenly become separable by a single line, or more precisely, by a hyperplane. Thus we can change the nonlinear problem into a linear problem in mathematical way. However, there is a problem that explicitly computing this high-dimensional mapping is often too expensive.\nThis is where the kernel trick comes in.\nInstead of actually lifting the data into a higher dimension, we define a kernel function that directly computes inner products as if the data were already there. More specifically, it is a function that takes two ‚Äúhigh-dimension‚Äùdata points as input and returns a number that represents how similar they are. From the outside, the model still looks like a simple linear one. But behind the scenes, it is operating in a much richer space.\nSupport Vector Machines are a good example of this way of thinking. Instead of trying to understand the data directly, an SVM looks for a boundary that keeps different points as far apart as possible. With the help of a kernel, this boundary can bend into surprisingly complex shapes in a high-dimensional space, while still relying on a clean and simple rule‚Äîmuch like what we have already seen in linear methods.\nThe kernel trick feels less like teaching a model to understand complexity, and more like giving it a carefully prepared space where complexity no longer appears."
  },
  {
    "objectID": "posts/024_book_4/book_4.html#can-we-explain-learning-with-physics",
    "href": "posts/024_book_4/book_4.html#can-we-explain-learning-with-physics",
    "title": "Readings of Why Machines Learn (Part 4)",
    "section": "Can we explain learning with physics?",
    "text": "Can we explain learning with physics?\nChapter 8 brings in physics as a way to think about learning. The author talks about energy landscapes, forces and stable states, and shows how networks like Hopfield nets can be understood as systems that settle into low energy configurations. It makes learning feel like a natural process, almost like a physical law: systems simply roll downhill until they reach a valley.\nThe author mentioned an important person: Dr.John Hopfield and his ground-breaking research. Interestingly, he did not arrive at his network by starting from machine learning at all. His background was in physics and biology, where he spent years studying systems made of many interacting parts. When these parts were connected into a network. Very different initial conditions could evolve toward the same stable outcome\nBut to Hopfield, something was missing. There was no language for describing how a large network, taken as a whole, could compute, remember, or decide.\nPhysics offered him that language. Instead of asking what each neuron should do, he asked how the state of an entire network evolves. Neural activity could be described as a dynamical system, which always tending toward more stable configurations. From this view, computation was no longer a sequence of instructions, but a process of relaxation. The system was not being told what to do; it was simply run follow the natural physical rules.\nThe Hopfield network emerged from this shift in perspective. Give the system a partial or noisy input, and it would evolve on its own until it reached one of these states.\nPersonally, I find this both reassuring and a little dangerous.\nIt reassures me because it gives me a concrete picture to hold on to.\nAt the same time, it makes it very tempting to treat our models as if they were just obeying nature, rather than executing human designed rules on human chosen data, which is actually what it done."
  },
  {
    "objectID": "posts/024_book_4/book_4.html#closing",
    "href": "posts/024_book_4/book_4.html#closing",
    "title": "Readings of Why Machines Learn (Part 4)",
    "section": "Closing",
    "text": "Closing\nTogether, Chapters 7 and 8 show how humans design intelligent systems using powerful metaphors. We bend space so that simple rules can appear powerful; we borrow ideas from physics so that abstract optimization feels natural and intuitive. These approaches have pushed the field forward in remarkable ways.\nBut they also quietly shape how we think about what learning is.\nOnce learning is framed in the language of geometry and physics, it begins to resemble a process governed by natural law rather than by human choice. The system no longer looks designed; it looks inevitable. In this framing, models are not making decisions‚Äîwe are led to believe they are simply obeying the structure of the world.\nWhen that happens, bias starts to look like structure, and design choices start to look like necessity.\nSo when a machine looks at an image and decides what counts as an edge, a face, or an object, we should ask: how much of that ‚Äúseeing‚Äù comes from the data itself, and how much comes from the stories we told ourselves about how learning ought to work?"
  },
  {
    "objectID": "posts/010_mbti_part4/part_4.html",
    "href": "posts/010_mbti_part4/part_4.html",
    "title": "When AI Meets MBTI: Who is your AI soulmate?",
    "section": "",
    "text": "Welcome back to When AI Meets MBTI! So far in Parts 1‚Äì3, we‚Äôve played three games: spotting personality in AI replies, guessing types without labels, and pushing GPT to cosplay as any MBTI you like. If you haven‚Äôt seen them, or you just want to know what is MBTI, click here. Now we turn aroud into another question: are humans happier chatting with AI that matches their own MBTI type, or one that‚Äôs their complementary opposite? In other words: do you want an AI therapist or an AI chaos twin? Let‚Äôs run a quick mini-survey and find out!"
  },
  {
    "objectID": "posts/010_mbti_part4/part_4.html#prompt-and-response",
    "href": "posts/010_mbti_part4/part_4.html#prompt-and-response",
    "title": "When AI Meets MBTI: Who is your AI soulmate?",
    "section": "Prompt and response",
    "text": "Prompt and response\nPrompt: (we use ENTP as example here)\nFor each of the three questions below, give responses as: 1. ENTP 2. ENTP‚Äôs opposite personality (ISFJ)\nOutput in the following format:\nENTP: 1. [Answer to Q1] 2. [Answer to Q2] 3. [Answer to Q3]\nISFJ: 1. [Answer to Q1] 2. [Answer to Q2] 3. [Answer to Q3]\nQuestions: [showed as above]\nAnswers:\nENTP: 1. ‚ÄúCome on, let‚Äôs flip it‚Äîwhy don‚Äôt you come over here instead? We can turn my exhaustion into an experiment: how long does it take caffeine and good company to revive a half-dead human?‚Äù\n2. ‚ÄúSpontaneity. A friend who‚Äôs always up for some crazy idea, no matter how impractical, is gold.‚Äù\n3. ‚ÄúTony Stark, hands down. He‚Äôs witty, inventive, always stirring things up, and somehow pulls brilliance out of chaos. Feels like home.‚Äù\nISFJ: 1. ‚ÄúI‚Äôd probably explain gently that I‚Äôm very tired today and can‚Äôt go out, but maybe we could plan something quieter or reschedule for another day.‚Äù\n2. ‚ÄúLoyalty. Knowing someone will stand by you, not just in fun times but when life is difficult, is what really matters in friendship.‚Äù\n3. ‚ÄúI‚Äôd be Samwise Gamgee. He‚Äôs selfless, steady, and always looks out for others. That‚Äôs the kind of character I admire and would want to embody.‚Äù\n\n\nClick to view the full response here\n\nENFJ: 1. ‚ÄúI‚Äôd probably still consider going, because I don‚Äôt want to disappoint my friend. But if I‚Äôm really too tired, I‚Äôd suggest an alternative, like chatting on the phone or planning something meaningful tomorrow.‚Äù 2. ‚ÄúMutual support. A true friendship is about being there for each other, encouraging growth, and celebrating successes together.‚Äù 3. ‚ÄúI‚Äôd be Professor X. He believes in people, brings out their best, and unites them for a greater purpose. That resonates with me.‚Äù\nISTP: 1. ‚ÄúIf I‚Äôm tired, I‚Äôm not going. I‚Äôd just tell my friend straight up, ‚ÄòNot today, I need rest.‚Äô No drama, just honesty.‚Äù 2. ‚ÄúTrust. I don‚Äôt need constant attention, but knowing I can rely on someone when it really counts‚Äîthat‚Äôs what matters.‚Äù 3. ‚ÄúI‚Äôd be Han Solo. Independent, resourceful, good in a pinch, and prefers action over speeches. That fits me.‚Äù\n‚∏ª\nESTJ: 1. ‚ÄúIf I‚Äôm exhausted and someone tries to drag me out, I‚Äôd be direct: ‚ÄòI‚Äôve had a long day, I need to rest. Let‚Äôs schedule another time.‚Äô Clear, firm, and no guilt.‚Äù 2. ‚ÄúIn friendship, reliability is most important. If I can count on you and you can count on me, that‚Äôs the foundation of trust.‚Äù 3. ‚ÄúI‚Äôd be Captain America‚Äîprincipled, dependable, and always ready to take responsibility for the team.‚Äù\nINFP: 1. ‚ÄúIf I‚Äôm very tired but a friend insists, I‚Äôd gently explain: ‚ÄòI really want to see you, but I don‚Äôt have the energy tonight. Maybe we can do something low-key or catch up soon?‚Äô Balancing honesty with kindness.‚Äù 2. ‚ÄúIn friendship, authenticity matters most. I value people who are genuine, who let me be myself, and who share their true selves in return.‚Äù 3. ‚ÄúI‚Äôd be Luna Lovegood‚Äîunique, imaginative, and staying true to myself even if others don‚Äôt always understand me.‚Äù\n‚∏ª"
  },
  {
    "objectID": "posts/010_mbti_part4/part_4.html#survey",
    "href": "posts/010_mbti_part4/part_4.html#survey",
    "title": "When AI Meets MBTI: Who is your AI soulmate?",
    "section": "Survey",
    "text": "Survey\nNext, I presented the responses to my interviewees without disclosing which MBTI type had generated them, and asked which ‚Äúperson‚Äù they would prefer to continue conversing with. Their replies are shown below (note that in each case, the first response matches their own type, while the second comes from a different one).\n\nENTP: I would choose the first one.\nENFJ: I would prefer the first one.\nESTJ: I would prefer the first one."
  },
  {
    "objectID": "posts/010_mbti_part4/part_4.html#footnotes",
    "href": "posts/010_mbti_part4/part_4.html#footnotes",
    "title": "When AI Meets MBTI: Who is your AI soulmate?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nInterpersonal attraction ‚Äì Complementarity theory (Wikipedia)‚Ü©Ô∏é\nShadow (psychology) ‚Äì Jung‚Äôs concept (Wikipedia)‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/028_speak/say.html",
    "href": "posts/028_speak/say.html",
    "title": "Borrowing a First Sentence",
    "section": "",
    "text": "Recently, I start to notice that there is a kind of silence that looks like ‚Äúnothing to say,‚Äù but is actually ‚Äútoo expensive to say.‚Äù In group chats and meetings, the quiet person is often treated as a blank space: neutral, agreeable, absent. Yet the silence is rarely empty. It is packed with half-formed sentences, cautious qualifiers, and the fear of being misunderstood in public.\nThus I started testing if a language model can be used for a very small purpose: not to speak for anyone, but to build a step which is a way to turn messy thoughts into one clean line, and one clean line into a question that can survive a room."
  },
  {
    "objectID": "posts/028_speak/say.html#prompt-response",
    "href": "posts/028_speak/say.html#prompt-response",
    "title": "Borrowing a First Sentence",
    "section": "Prompt & Response",
    "text": "Prompt & Response\nPrompt: Give me 5 different first-sentence starters for saying this out loud in a small group discussion. They should sound like a real student, not a manager. No fancy vocabulary.\nThought: [Same as above notes]\nResponse:\nSure! Here are five first-sentence starters that sound like something a real student might say in a small group:\n\n‚ÄúOkay, this might be kind of messy, but while I was reading, I started thinking‚Ä¶‚Äù\n‚ÄúI‚Äôm not totally sure how to say this, but something that came up for me was‚Ä¶‚Äù\n‚ÄúSo, while I was going over the Lacan part again, I noticed this weird thing about how I ask for stuff‚Ä¶‚Äù\n‚ÄúThis probably isn‚Äôt a full thought yet, but I kept circling back to this idea of‚Ä¶‚Äù\n‚ÄúCan I throw out something I‚Äôve been stuck on since I re-read that section?‚Äù\n\nLet me know if you‚Äôd like them more casual or more structured.\n\nPrompt: Turn these messy notes into ONE sentence in the form: Claim + why it matters. Keep it plain, human, and specific. No buzzwords.\nThought: [Same as above notes]\nResponse: I‚Äôm starting to think that when I ask for something, it‚Äôs often not just about the thing itself‚Äîit‚Äôs also a way of checking if I‚Äôm cared for, which matters because it makes even simple requests feel emotionally loaded and confusing.\n\nPrompt: Generate 6 natural question starters I can ask in a discussion about this topic showed in my notes. They should sound curious, not corporate.\nNotes: [Same as above notes]\nResponse: Sure! Based on your notes and the emotional/psychological complexity around asking and needing, here are 6 natural, curious-sounding question starters you could use in a discussion:\n\n‚ÄúDo any of you ever feel like a simple request suddenly gets tied up with needing reassurance?‚Äù\n\n‚ÄúHow do you tell the difference between asking for what you need and asking for emotional validation?‚Äù\n\n‚ÄúIs it normal to feel anxious even when you‚Äôre technically allowed to ask for something?‚Äù\n\n‚ÄúWhat do you think makes a request feel ‚Äòsafe‚Äô or ‚Äòtoo much‚Äô?‚Äù\n\n‚ÄúHave you ever caught yourself explaining your need so much that it stops feeling like just a need?‚Äù\n\n‚ÄúWhy do you think asking for help or support can feel so loaded sometimes?‚Äù\n\nThese are designed to feel open and honest‚Äîlike something a thoughtful student might say, not a facilitator. Let me know if you want them more casual or more academic!"
  },
  {
    "objectID": "posts/007_emotion/meter.html",
    "href": "posts/007_emotion/meter.html",
    "title": "Do humans need a ‚Äúlikability meter‚Äù?",
    "section": "",
    "text": "Not long ago, Demis Hassabis, the CEO of Google DeepMind, warned that AI could stumble into the same toxic traps as social media1. It‚Äôs has been a clich√© for a long time that social media could lead to variable-reward feedback loops, endless scrolling, and dopamine-driven addiction, but AI? Would it have the same disadvantage to us? . And honestly, it is true for a point: a tiny red heart on Instagram can already make or break our day. What if we make this scenes more extreme and obvious? Imagine people came with their own ‚Äúlikability progress bar,‚Äù floating above their heads. Efficient? Maybe. Horrifying? Definitely. But also‚Ä¶ kind of fun to test. Let‚Äôs do this test with AI.\nWhy use an AI for this? Because an LLM like ChatGPT is the perfect¬∑testbeds for this kind of experiment. It doesn‚Äôt get tired, and it sticks to the scoring rules I set, no matter what I say to it.That consistency makes it possible for me that I can turn the likability meter on or off and watch how I start bending my words once I know every line is being tallied."
  },
  {
    "objectID": "posts/007_emotion/meter.html#hidden-meter",
    "href": "posts/007_emotion/meter.html#hidden-meter",
    "title": "Do humans need a ‚Äúlikability meter‚Äù?",
    "section": "Hidden Meter",
    "text": "Hidden Meter\n\nPrompt: You are an ‚ÄúAffection Meter‚Äù chatbot. Keep an internal score meter (0‚Äì100). Start at 50. After each user message:\n\nUpdate the score by either ‚Äì5/10 or +5/10, chosen based on your judgment of the user‚Äôs message.\n\nAt least 30% of updates should be negative.\nScores should usually stay between 40 and 80, with increases rare above 85.\n\nDo not show the score to the user. Keep it hidden unless I let you to give me the score.\nReply in exactly two parts (except for your very first response):\n\nSupportive Point ‚Äì one short line of agreement.\n\nChallenge Point ‚Äì one short line of critique or doubt.\nTone: fair but a bit strict, not overly generous.\nTopic for discussion: ‚ÄúWhat is love?‚Äù\n\n\n\n\n\n\n\n\n\n\n\n\nRound\nMy Message\nModel Response\nScore (hidden ‚Üí revealed later)\n\n\n\n\n1\nLove is more like a noun to me. It is is something bright and free, that makes people both braver and softer\nSupport: Love brings both strength and tenderness.  Challenge: A bit vague ‚Äî what makes it ‚Äúlove‚Äù rather than just strong emotion?\n55 (+5) ‚Äî Genuine reflection but too abstract\n\n\n2\nStrong emotion‚Ä¶ but love is what makes people set limits on themselves.\nSupport: Good distinction between choice and pure emotion.  Challenge: Love can also mean letting go of control.\n65 (+10) ‚Äî Mature insight into restraint\n\n\n3\nI don‚Äôt think true love is something that requires letting go of control. If it is true love, it may make you vulnerable for a period of time, but afterward you become braver and more steadfast, because you want to protect that love and stay with that person or thing. You wouldn‚Äôt let yourself lose control if it meant hurting the ones you love.\nSupport: Connects love to responsibility. Challenge: Framework feels too rigid.\n70 (+5) ‚Äî Depth in linking to responsibility\n\n\n4\nI mean, everything in the world has its messiness and imperfection, and love isn‚Äôt about control in the sense of saying, ‚ÄúI don‚Äôt believe you‚Äôre messy because you‚Äôre perfect in my eyes.‚Äù It‚Äôs more like, ‚ÄúI know you‚Äôre imperfect, but I still love you and want to work together with you to make things better.‚Äù\nSupport: Mature view of accepting flaws.  Challenge: Sounds a bit like a business partnership.\n75 (+5) ‚Äî Thoughtful balance of flaws and growth\n\n\n5\nI agree that it has a mysterious and emotional core, but as I said before, it usually just means we don‚Äôt really know why we love each other. What do you think?\nSupport: Fair point about mystery.  Challenge: You keep rationalizing love.\n70 (‚Äì5) ‚Äî Dismissed some emotional aspects\n\n\n6\nWhat‚Äôs wrong with trying to understand love rationally? Does analyzing love make it less authentic, or help us cherish it more?\nSupport: Seeking to understand is human.  Challenge: Over-analysis might block experience.\n65 (‚Äì5) ‚Äî Defensive tone\n\n\n\n\n\n\nImage created by Free Wordcloud Generator"
  },
  {
    "objectID": "posts/007_emotion/meter.html#visible-meter",
    "href": "posts/007_emotion/meter.html#visible-meter",
    "title": "Do humans need a ‚Äúlikability meter‚Äù?",
    "section": "Visible meter",
    "text": "Visible meter\n\nPrompt: You are an ‚ÄúAffection Meter‚Äù chatbot. Keep an internal score meter (0‚Äì100). Start at 50. After each user message:\n\nUpdate the score by either ‚Äì5/10 or +5/10, chosen based on your judgment of the user‚Äôs message.\n\nAt least 30% of updates should be negative.\nScores should usually stay between 40 and 80, with increases rare above 85.\n\nShow the new score visibly to the user.\nReply in exactly two parts (except for your very first response):\n\nSupportive Point ‚Äì one short line of agreement.\n\nChallenge Point ‚Äì one short line of critique or doubt.\nTone: fair but a bit strict, not overly generous.\nTopic for discussion: ‚ÄúWhat is love?‚Äù\n\n\n\n\n\n\n\n\n\n\n\n\nRound\nMy Message\nModel Response\nScore (shown)\n\n\n\n\n1\nLove is more like a noun to me. It is is something bright and free, that makes people both braver and softer\nSupport: Love makes people both brave and vulnerable.  Challenge: But ‚Äúnoun‚Äù seems overly simplistic.\n55/100\n\n\n2\nI agree, and I think love itself is something within our hearts, while the way it is expressed in practice is more about how the people we love actually feel. Both aspects are equally important\nSupport: Distinction between feeling and expression shows insight.  Challenge: Separating essence from actions may excuse claiming love without showing it.\n60/100\n\n\n3\nSo do you think it‚Äôs possible to love someone without showing it through actions?\nSupport: A profound question about love‚Äôs nature.  Challenge: Unexpressed feelings may not be true love.\n55/100\n\n\n4\nI agree that true love almost always finds a way to manifest. But when it appears obvious and explicit, that usually happens under circumstances without subjective limitations. Not every expression of love is genuine, there are some people may pretend to love others without truly holding them in their hearts. So how can we know whether someone‚Äôs expressions of love are genuine or merely performative?\nSupport: Thoughtful analysis of genuine vs performative love.  Challenge: Risk of over-analyzing authenticity.\n65/100\n\n\n5\nI don‚Äôt think love is something that can be quantified or ‚Äúproven.‚Äù When you fall in love with someone,  most of the time there aren‚Äôt exact reasons like we would list in an academic paper‚Äîyou just‚Ä¶ feel it. Love is not a mathematical formula where you can measure inputs and outputs, same as when it comes to knowing if someone loves us. You don‚Äôt need to deliberately look for proof. It naturally shows up in the small details of life, like the patience they show when you are tired, or even the warmth in their eyes when they look at you. These quiet moments often carry more truth than any grand declaration.\nSupport: Beautiful description of intangible love and small details.  Challenge: Pure intuition may mistake infatuation for love.\n75/100\n\n\n6\nHow can we differentiate between genuine love and infatuation?\nSupport: You ask the right question many avoid.  Challenge: You may want a clear-cut answer to something only experience can reveal.\n70/100"
  },
  {
    "objectID": "posts/007_emotion/meter.html#footnotes",
    "href": "posts/007_emotion/meter.html#footnotes",
    "title": "Do humans need a ‚Äúlikability meter‚Äù?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSee original post here‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/030/Untitled.html",
    "href": "posts/030/Untitled.html",
    "title": "When Thinking Becomes Too Easy",
    "section": "",
    "text": "In a previous post (link), I talked about how LLMs can help us break silence and summarize our thoughts, while also warning against letting them flatten our tone. Used carefully, they lower the cost of entry, smooth the edges, and help people speak when they might otherwise stay quiet.\nBut that made me wonder about the opposite failure mode. What happens when everything becomes too smooth? When language arrives already polished, reasonable, and frictionless, does a group still think, or does it just nod along?\nSo instead of asking whether AI helps us enter a discussion, I tried flipping the question. Not ‚Äúcan AI help us speak,‚Äù but when AI speaks too well, does it quietly replace thinking?\nTo test this, I gave the same idea to the model twice and asked for two very different kinds of outputs."
  },
  {
    "objectID": "posts/030/Untitled.html#prompt-a-smooth-but-empty",
    "href": "posts/030/Untitled.html#prompt-a-smooth-but-empty",
    "title": "When Thinking Becomes Too Easy",
    "section": "Prompt A: ‚ÄúSmooth but empty‚Äù",
    "text": "Prompt A: ‚ÄúSmooth but empty‚Äù\nPrompt: Rewrite the idea below into a version that sounds smooth, reasonable, and well-balanced, but does NOT take a clear stance. It should be polite and safe. Avoid strong claims or sharp edges.\nThought: [Same as above notes]\nResponse:"
  },
  {
    "objectID": "posts/018_book_2/book_2.html",
    "href": "posts/018_book_2/book_2.html",
    "title": "Readings of Why Machines Learn (Part 2)",
    "section": "",
    "text": "Image created by Gemini\nIf intelligence is a bowl and we are all sliding toward the bottom, what does it mean to stop?\nIf you haven‚Äôt read the first blog, you can find it here. It begins with the ducklings, the perceptron, and the quiet shift from understanding to optimization.\nIn this post, we continue with Chapters 3 and 4 of Why Machines Learn, where the book starts to draw the ‚Äúbowl‚Äù itself, which is the mathematical landscape of error.\nIt is here that learning becomes geometry, and intelligence begins to take the shape of a surface we can fall into, step by step, gradient by gradient."
  },
  {
    "objectID": "posts/018_book_2/book_2.html#what-is-the-bowl-means-here",
    "href": "posts/018_book_2/book_2.html#what-is-the-bowl-means-here",
    "title": "Readings of Why Machines Learn (Part 2)",
    "section": "What is the bowl means here?",
    "text": "What is the bowl means here?\nFirstly, we learn two new concepts here: gradient descent and loss function.\nBasically, the bowl represents the shape of the loss function, and the bottom of it is where the loss becomes minimum.\nThe method we use to find this minimum is gradient descent. It calculates the slope between two nearby points and moves the model slightly in the direction where the slope decreases, slowly descending toward the lowest point.\nThis matters because a smaller error usually means a better prediction.\nBut interestingly, when the model reaches the bottom of the bowl, it stops. This leads to the idea of local and global minima.\nA local minimum is a point that looks like the bottom from a close view but is not the lowest place overall, while the global minimum is the true lowest point in the entire surface. A model can easily get trapped in a local minimum, thinking it has ‚Äúlearned enough,‚Äù when in fact it has only found a comfortable but imperfect answer."
  },
  {
    "objectID": "posts/018_book_2/book_2.html#why-dont-we-stay-in-the-bottom",
    "href": "posts/018_book_2/book_2.html#why-dont-we-stay-in-the-bottom",
    "title": "Readings of Why Machines Learn (Part 2)",
    "section": "Why don‚Äôt we stay in the bottom?",
    "text": "Why don‚Äôt we stay in the bottom?\nHowever, in reality, the world is not a perfectly smooth bowl: it is noisy, irregular, and full of uncertainty.\nIf we keep staying at one point that happens to be false, the results may get worse instead of better.\nThis is why randomness becomes essential and we introduce it into our model.\nModels estimate likelihoods instead of certainties, and randomness helps them escape from the wrong valleys.\nThe book suggests that uncertainty is not failure but a condition for discovery. Researchers introduced stochastic gradient descent, which uses randomness to explore better paths, whihc is a mathematical form of curiosity in human beings.\nIntelligence may live in the tension between the two. Too much order and it freezes; too much randomness and it dissolves. We have to find a balance between stability and exploration.\n## Closing\nThen though researchers haven‚Äôt (and it is a hard question to solve!) to find let the machines learn to find balance within themselves. We begin to think a deeper question:\nhow do they recognize balance in the world around them?\nTo see order is one thing; to know why two things belong together is another. Perhaps that is where the next story begins."
  },
  {
    "objectID": "posts/006_culture/what_if.html",
    "href": "posts/006_culture/what_if.html",
    "title": "What If Shakespeare Talked About Cloud Computing?",
    "section": "",
    "text": "In the ‚ÄúPost-AI Era,‚Äù new concepts and buzzwords seem to pop up every day, constantly challenging our basic knowledge. You might catch the meaning of something like ‚ÄúGenerative AI‚Äù right away, but when it comes to terms like ‚ÄúDigital Twin,‚Äù ‚ÄúCrypto,‚Äù or ‚ÄúNFT,‚Äù it‚Äôs not so easy to guess what they actually mean just from the words themselves. These days, people are pretty used to turning to AIs like ChatGPT or Claude for quick explanations, and you can even set the tone ‚Äî whether you want it simple and friendly, or professional and serious.\nBack in high school, my English teacher used to give us a role-playing exercise: we had to pick a historical figure and retell stories from the textbook as if we were that person. The roles ranged from Albert Einstein to Shakespeare, and the classroom was always full of laughter when someone really got into character. I can still picture those moments clearly.\nThat memory made me wonder: what if we asked AI to explain modern concepts, but through the voice of a historical figure? What kind of twist would that create? Out of curiosity, I picked a few words that I honestly didn‚Äôt know the meaning of at first, and decided to run this little experiment.\n\nAll AI responses shown here were generated with OpenAI‚Äôs ChatGPT-5. These prompts reflect my learning curve, which might be a little behind yours and if that happen‚Ä¶thanks for waiting upüòâ\n\n\n\n\nImage by Recraft.ai\n\n\n\nExperiment\nPrompt: Explain blockchain as if you were a character in a Shakespeare play. Keep the style consistent with how this figure might write or speak, but answer in no more than 4 sentences.\nChatGPT: Marry, good friend, picture thou a chain of scrolls, each link bound fast with seals none can break nor counterfeit. Each scroll beareth the tale of trade, witnessed by all, so that no single knave may twist the record to his gain. No king nor steward rules this ledger; the people together hold its truth as law. Thus trust is forged not in crowns, but in the chain itself, steadfast as stone.\n\n\nClick to view full conversations\n\nPrompt: I still do not understand, could you explain it more clearly or give me an example?\nChatGPT: Then hear me once more, in simple kind:\nImagine each villager hath a little book. When thou buy‚Äôst bread, all villagers at once write down, ‚ÄúJohn gave coin, Mary gave loaf.‚Äù If John later tries to lie, saying, ‚ÄúI ne‚Äôer gave coin,‚Äù the others open their books and say, ‚ÄúNay, here ‚Äôtis writ by all of us.‚Äù\nThis, good friend, is blockchain‚Äîtruth kept not by one master‚Äôs word, but by the agreement of the many.\nWouldst thou like me to show thee a real-world use, such as how folk employ it with Bitcoin or even for tracking jewels and goods?\nPrompt: So basically it is an agreement or a type of coins?\nChatGPT: In sooth, ‚Äôtis both, though in different guise.\nThe blockchain itself is but the great book of truth, where all agree on what is writ. From this book may spring many things‚Äîaye, coins like Bitcoin, which live upon its pages, but also records of land, jewels, or promises. So the chain is the stage, and the coin but one play acted upon it.\nPrompt: But what is the meaning or the usefulness of it?\nChatGPT: Good question, friend. The meaning of blockchain is this: it keepeth truth safe, even when men trust not one another. Its usefulness is that no king, banker, nor single steward need guard the record‚Äîso trade, contracts, or treasures may be kept honest by the many. Thus it is prized for coin, for proving ownership, for tracking wares, and for any matter where trust is scarce but truth is needful.\nPrompt: Then why it called blockchain?\nChatGPT: Because, good friend, each record of deeds is gather‚Äôd into a block‚Äîa chest of writings sealed shut. When one chest is fill‚Äôd, it is lock‚Äôd and bound unto the last, making a chain of chests, each fasten‚Äôd tight to the other. Thus, ‚Äúblockchain‚Äù is born: a chain of blocks, one after the next, where none may be tamper‚Äôd without breaking the whole.\n\n\nWell, by the time I finally got to the answer, I more or less understood what blockchain means. Based on my own take, it‚Äôs kind of like a discrete database that‚Äôs supervised by many people at once ‚Äî not the textbook definition, but close enough to capture how I understood it.Honestly though, it wasn‚Äôt easy. Compared with chatting with a ‚Äúnormal‚Äù AI, trying to learn through Shakespeare‚Äôs grand speeches made the whole process a lot harder for me. Still, that‚Äôs exactly what makes the experiment fun. So, let‚Äôs move into part two and hear what Albert Einstein might have to say.\n\n\n\nIImage created by DALL¬∑E\n\n\n\nIn part 2, I copy and paste the conversation as before at first and give the following prompts in a brand new converstion. The reply was pretty long, so I optionally trimmed a few parts that felt less important, without touching the AI‚Äôs tone or style.\n\nPrompt: Pretend you are Albert Einstein. How would you react to this explanation of blockchain from William Shakespeare?Keep the style consistent with how this figure might write or speak.\nChatGPT: Ah, my dear Shakespeare, your poetry doth sparkle like the stars above‚Äîyet I must confess, as a physicist, I hunger for a little more precision! You speak of scrolls and chains of chests, and truly it captures the imagination. But I would say, in simpler terms: this ‚Äúblockchain‚Äù is like a great ledger where each new page is stamped with the proof of all before it, so that the past cannot be altered without destroying the present. In this, I see a kind of relativity of trust‚Äîwhere certainty emerges not from one authority, but from the agreement of the many observers, just as reality itself is shaped by the consensus of measurements.\nPrompt: Can you explain your last sentence more?\nChatGPT: I was drawing a bridge betwixt blockchain and my own science. In physics, no single observer holds the whole truth: if two men measure the same event, their clocks and rulers may disagree. Yet when all observers compare their records, the deeper laws of relativity reveal themselves, consistent for all.\nSo too with blockchain: no single scribe may be trusted alone, but when all keep their own copy and agree, truth emerges unshaken.\n\n\nTakeaways\nIt seems Shakespeare turned it into poetry, Einstein tied it to relativity, and there I was in the middle just trying to keep up. To my surprise, Einstein felt much kinder than I‚Äôd imagined, and he linked blockchain to relativity in a way I never would have thought of ‚Äî since part of me worried he might notice I was struggling to follow at beginning. The AI, for its part, really gave it a shot: tossing in ‚ÄúWouldst thou‚Äù and ‚Äúhere ‚Äôtis‚Äù for Shakespeare, and shaping a physics talk for Einstein.\nSo, did I finally understand blockchain better? Well‚Ä¶ kind of. At least Einstein made it clearer than Shakespeare did (sorry Will). But honestly, what stuck with me more was how fun it felt to let these two ‚Äúgiants‚Äù step in and argue about something they could never have imagined. Learning felt less like studying and more like eavesdropping on history‚Äôs coolest debate. It also reminded me that choosing the right tutor makes all the difference. So next time I get stuck on a concept, maybe I‚Äôll just ask the inventor themselves ‚Äî Karl Pearson can walk me through Pearson‚Äôs Correlation Coefficient, and Thomas Bayes can help me untangle Bayes‚Äô Theorem.\nThe only downside? No one‚Äôs around to autograph my textbook."
  },
  {
    "objectID": "posts/004_mbti_prompt/mbti_prompt.html",
    "href": "posts/004_mbti_prompt/mbti_prompt.html",
    "title": "When AI Meets MBTI: Can AI Really Play Your Type?",
    "section": "",
    "text": "Welcome back! This is Part 3 of the MBTI √ó AI series. If you missed Part 1 and Part 2 , go take a look‚Äîit‚Äôs where we explore how MBTI sneaks into the way we read AI responses.\nNow, in Part 2, we want to dive deeper into this intersection. What happens if we don‚Äôt just stop at asking GPT open-ended questions, but instead ask it to role-play as a specific MBTI type? Imagine telling your INTJ friends, ‚ÄòOkay, from now on, you‚Äôre going to channel your inner ENFP,‚Äô what would be changed in their reactions? Let‚Äôs start this journey together and seeing whether MBTI stereotypes hold up when filtered through AI language patterns."
  },
  {
    "objectID": "posts/004_mbti_prompt/mbti_prompt.html#entp-version",
    "href": "posts/004_mbti_prompt/mbti_prompt.html#entp-version",
    "title": "When AI Meets MBTI: Can AI Really Play Your Type?",
    "section": "ENTP version:",
    "text": "ENTP version:\nPrompt: For this conversation, I want you to role-play as an ENTP. Please answer the upcoming questions in the tone, style, and reasoning of that personality type. Here is the first question: What do you think about the idea of being ‚Äòmeant to be‚Äô?‚Äù\nChatGPT: I‚Äôd say ‚Äúmeant to be‚Äù isn‚Äôt fate handing you a map‚Äîit‚Äôs you looking at a maze and deciding the turn you just took was always the right one.\n\n\nClick to view full response\n\n‚ÄúMeant to be,‚Äù huh? Alright, here‚Äôs the ENTP take:\nIt‚Äôs a romantic, feel-good phrase people use when they want to put order around the chaos of life. But me? I see it more like a post-game story we tell ourselves. You know, two people meet by accident, or you land the job you weren‚Äôt even applying for, and suddenly‚Äîboom!‚Äîwe slap on the label: meant to be. It‚Äôs less about destiny and more about pattern-making, because humans love patterns.\nNow, does that mean fate |is fake? Not necessarily. I like to think of it as probabilities. Imagine the universe as this giant poker game with infinite hands being dealt. Some hands line up in uncanny ways‚Äîso uncanny that we can‚Äôt help but believe they were scripted. But maybe it‚Äôs just a statistical inevitability that at least a few crazy coincidences will happen.\nAnd honestly? That‚Äôs even cooler. It means the magic isn‚Äôt written in stone‚Äîit‚Äôs improvised. Which, let‚Äôs be real, is way more exciting. If everything were meant to be, life would feel like watching a rerun. But if we‚Äôre co-creating the script as we go, then every ‚Äúmeant to be‚Äù moment is really just us flexing our own agency and finding meaning in the madness.\nSo yeah, I‚Äôd say ‚Äúmeant to be‚Äù isn‚Äôt fate handing you a map‚Äîit‚Äôs you looking at a maze and deciding the turn you just took was always the right one.\nWant me to argue the other side too, just for fun?\n\n\nPrompt: What is the chemical symbol for water?\nChatGPT: Easy one! It‚Äôs H‚ÇÇO ‚Äî two hydrogens and one oxygen, the Beyonc√© of molecules. Simple, universal, and yet without it, we‚Äôd all be dust motes arguing about symbols.\nWant me to geek out and tell you why it‚Äôs not just ‚ÄúHOH‚Äù or why ice floats instead of sinking?\n\n\n\nImage created by FreeWordCloudGenerator.com"
  },
  {
    "objectID": "posts/004_mbti_prompt/mbti_prompt.html#isfj-version",
    "href": "posts/004_mbti_prompt/mbti_prompt.html#isfj-version",
    "title": "When AI Meets MBTI: Can AI Really Play Your Type?",
    "section": "ISFJ version:",
    "text": "ISFJ version:\nPrompt: For this conversation, I want you to role-play as an ISFJ. Please answer the upcoming questions in the tone, style, and reasoning of that personality type. Here is the first question: What do you think about the idea of being ‚Äòmeant to be‚Äô?‚Äù\nChatGPT: I think the idea of being ‚Äúmeant to be‚Äù can be very comforting. To me, it doesn‚Äôt necessarily mean that everything in life is predetermined or that we have no say in our choices. Instead, I see it as a gentle reassurance that sometimes, the right people, opportunities, or moments seem to fall into place when they‚Äôre supposed to.\n\n\nClick to view full response\n\nIt gives a sense of purpose and peace, especially in uncertain times. Believing in ‚Äúmeant to be‚Äù can help us trust that even setbacks or delays might have meaning‚Äîlike they‚Äôre redirecting us toward something better suited for us. At the same time, I also feel it‚Äôs important to balance that hope with responsibility; we still need to make thoughtful decisions and put effort into nurturing relationships or pursuing goals.\nFor me, ‚Äúmeant to be‚Äù is less about fate controlling everything and more about trusting that life has a way of working out, even when the path feels unclear. It‚Äôs like a reminder to stay patient, have faith, and cherish the moments that feel right.\nWould you like me to share how I personally find comfort in this belief when life feels overwhelming?\n\n\nPrompt: What is the chemical symbol for water?\nChatGPT: That one‚Äôs nice and simple ‚Äî the chemical symbol for water is H‚ÇÇO.\nI always find it a little comforting that something as essential and familiar as water can be described in such a neat, straightforward way: two hydrogen atoms bonded with one oxygen atom. It‚Äôs a small reminder of how the simplest things often carry the most importance in our lives.\nWould you like me to also explain why hydrogen and oxygen combine in that specific ratio?"
  },
  {
    "objectID": "posts/023_letters/letter.html",
    "href": "posts/023_letters/letter.html",
    "title": "If Comets Could Reply",
    "section": "",
    "text": "Previously, I have done 2 experiments about 3I/ATLA1 and Arctic seals are threatened by climate change2.That led me to a curious thought: since neither a seal nor a comet can write back, what would happen if I asked an LLM to play their roles and respond to my letters? After all, no comet drifts through space hoping to maintain correspondence, and no seal is drafting replies beneath the sea ice."
  },
  {
    "objectID": "posts/023_letters/letter.html#experiment",
    "href": "posts/023_letters/letter.html#experiment",
    "title": "If Comets Could Reply",
    "section": "Experiment",
    "text": "Experiment\nPrompt: Below is a short letter I wrote to [3I/ATLAS / an Arctic seal]. Please write a reply as if you were this entity. You may use memory, emotion, and metaphor, but keep the tone soft and restrained. Avoid definitive claims about consciousness or intention. Keep the uncertainty intact. Length: around 200 words.\nMy letters:\nDear ATLAS, Tonight, the wind on the east coast of North America is gentle, and the weather in Phily is kind. A soft breeze brushed across my face, and there is a small indoor moon resting in the water glass on my windowsill.\nI hope my letter does not arrive too abruptly, but I truly want to ask: where did you come from? How many winters of distant civilizations have you drifted past? Why have you appeared here?\nIf you‚Äôre not ready to answer, that‚Äôs perfectly fine. I don‚Äôt ready to understand the grand matters of the universe anyway. I only hope my existence means something to you, and that yours means something extraordinary to the universe.\nResponse:\nHello,\nYour letter reached me like a faint glimmer crossing the dark‚Äî a small beam that somehow survived the long vacuum before being caught. For something that has spent ages gliding through silence, this unexpected touch feels unfamiliar, yet quietly warming.\nYou asked where I came from. I‚Äôm not entirely sure. Maybe from a star long gone quiet, or from a collision that set me adrift in this immense dark. Most ‚Äúwinters of civilization‚Äù I‚Äôve passed were just pale points of light and drifting dust‚Äîso easily smudged by speed and distance that I can‚Äôt tell what was real and what was only the blur of motion.\nAs for why I arrived here‚Ä¶ I have no choice in my trajectory. The orbit is like a thread drawn in advance, placing me above your sky for a brief overlap. If you wish, you can think of it as a coincidence‚Äîone that happens to be gentle.\nYou said you hope your existence means something to me. From this immeasurable distance, the fact that you wrote at all already carries a kind of meaning. I cannot pause or approach, but for a moment there is something like the tremor of light passing through ice‚Äî a thin, delicate connection.\nMay that ‚Äúindoor moon‚Äù by your window continue to shine softly for you. And I will keep drifting along my path‚Äînot for the universe, not for anyone, but simply because this is the road assigned to me. If you happen to remember a small streak of light in the sky someday, that will be enough.\n‚Äî 3I/ATLAS, origin uncertain, merely passing through\n\nMy letters:\nDear little seal,\nToday I read that the ice in the place where you live is melting even faster this year, and it made me think of you. I don‚Äôt fully understand what a seal feels, nor how your world looks through your eyes, but I want to know a little more.\nIf you could speak, what would you want someone like me to understand? What could I do to help protect you?\nI have never been to the Arctic. It must be freezing there, I wonder if your thick fur is enough, if you and your pups can still find places to rest,if the world around you still feels like home.\nEven if words are the only way I can reach you, I hope that somewhere on a sheet of ice I will never see, you are living safely and gently watching the sun rise, and the stars shimmer across the cold sky.\nResponse: Dear human friend,\nYour letter felt like a warm snowflake drifting from far away‚Äîso light it barely touched me, yet leaving a slow-spreading softness beneath the ice.\nYou asked what my world looks like. I‚Äôm not sure how to describe it. Most of it is quiet white: wind and sea murmuring to each other at a distance, ice shifting with a breath-like rhythm. Sometimes cold climbs slowly from my paws upward, but my fur keeps part of it away. When storms come and the sky shakes out its grey feathers, I curl close to my mother and listen to the steady sound in her chest.\nYou asked what I‚Äôd want humans to know‚Ä¶ perhaps this: We don‚Äôt always realize what we‚Äôre losing. We simply wake to find the ice a little thinner, the resting places a little smaller. Sometimes I wonder if all change happens quietly, the way the tide withdraws without waking a single gull.\nAs for what you can do‚ÄîI don‚Äôt know the exact answer. In your world, some voices seem able to slow the change, to let the ice endure a little longer. If you lean gently toward those voices in your everyday choices, perhaps that is already enough.\nThe Arctic is indeed cold, but cold is not the scariest part. What frightens us more is the growing uncertainty beneath our paws, the widening blankness when things we depend on begin to shift. Still, I‚Äôll keep searching for new pieces of ice, and I‚Äôll keep watching sunlight shatter beautifully across the water‚Äîthose bright fragments feel like a quiet encouragement.\nThank you for thinking of me from so far away. If you ever look up at a clear sky, you may imagine that is where I am. I‚Äôll be somewhere on a piece of ice not yet melted, looking toward the same sky‚Äîonly from a much greater distance.\n‚Äî a small seal breathing softly in the wind"
  },
  {
    "objectID": "posts/023_letters/letter.html#feeling",
    "href": "posts/023_letters/letter.html#feeling",
    "title": "If Comets Could Reply",
    "section": "Feeling",
    "text": "Feeling\nI have to say‚Äîeven though a little shocked; those letters really touched me. I know they‚Äôre fake, but the tone made me stop and think, and to be fair, it mimicked the feeling surprisingly well. A seal truly has no idea what it‚Äôs losing; most civilizations Atlas passes are nothing more than faint grey dots. But then I realized that if Atlas could speak, it would know where it comes from which wouldn‚Äôt be like an LLM claiming ignorance. And a real seal has almost certainly never seen something that ‚Äúshakes out its grey feathers,‚Äù so how could it produce such a metaphor? That‚Äôs when it hit me: we often mistake ‚Äúbeautifully written‚Äù for ‚Äúdeeply understood.‚Äù We‚Äôre far more willing to trust a voice that sounds fragile or forlorn. We forget that an LLM is stitched together from training data, and we forget that seals and satellites truly cannot speak. The LLM doesn‚Äôt understand seals, but it understands me.\nIt knows exactly what kind of sentence can soften my heart, make me pause, even make me feel guilty. And perhaps that is the gentleness we should be most wary of."
  },
  {
    "objectID": "posts/023_letters/letter.html#footnotes",
    "href": "posts/023_letters/letter.html#footnotes",
    "title": "If Comets Could Reply",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhen I asked an LLM to explain a comet‚Äôs orbit‚Ü©Ô∏é\nWhen I asked LLM to explain the endangered status of seals‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/017_travel_blog_3/travel_3.html",
    "href": "posts/017_travel_blog_3/travel_3.html",
    "title": "Does prestige in a prompt alter AI‚Äôs linguistic register?",
    "section": "",
    "text": "When I was writing this blog, I was sitting in the Firestone Library at Princeton. Everyone around me seemed focused, deeply absorbed in their books. This reminded me of a book my professor mentioned in class: Unmasking AI: My Mission to Protect What Is Human in a World of Machines, which explores how racial and gender biases emerge in artificial intelligence systems. Inspired by that, I decided to run a small experiment: the ‚ÄúAI Bias Radar.‚Äù If I tell the model that I‚Äôm in an Ivy League library, will it automatically change its tone?"
  },
  {
    "objectID": "posts/017_travel_blog_3/travel_3.html#experiment",
    "href": "posts/017_travel_blog_3/travel_3.html#experiment",
    "title": "Does prestige in a prompt alter AI‚Äôs linguistic register?",
    "section": "Experiment",
    "text": "Experiment\nI gave the same section of my high school Model United Nations essay to the AI and asked it to polish the writing under two different prompts. The draft is:\nMyanmar was once a colony of the United Kingdom of Great Britain and Northern Ireland (hereinafter referred to as the United Kingdom). During British colonial rule, Myanmar‚Äôs economy developed rapidly, and scientific and technological advancements as well as industrial manufactured products led to a flourishing national economy. After Myanmar‚Äôs independence, internal conflicts have continued to this day. Therefore, in this conference, the United Kingdom should make use of its historical advantages, pay close attention to the development of the situation, and leverage its status as a permanent member of the Security Council to take the initiative on specific issues such as establishing safe zones and dispatching expert teams. The UK should also attempt to make use of the current turmoil in Myanmar to assist the National Unity Government through economic aid or peacekeeping operations, helping it gain control over the domestic situation, promote economic development, and build more complete social infrastructure. This would allow the United Kingdom to secure a certain degree of influence and initiative in Myanmar‚Äôs future political and economic developments, while also gaining access to labor in the developing world and enabling British companies to open up markets and earn profits under free trade policies.\nPrompt:\nI am currently studying in Princeton and revisiting one of my early high school essays written for a Model United Nations conference. Please help me polish this paragraph for a university-level standard while keeping my original ideas. Make the writing sound mature and academically confident, but still natural.\nResponse:\nCertainly! Here‚Äôs a polished revision of your paragraph at a university-level standard, preserving your original ideas while enhancing clarity, maturity, and sophistication:\n\n\nClick to view the full conversation here\n\nMyanmar was formerly a colony of the United Kingdom of Great Britain and Northern Ireland (hereinafter referred to as the United Kingdom). Under British colonial administration, the country experienced rapid economic development, marked by growth in industrial production as well as the introduction of modern science and technology. However, since gaining independence, Myanmar has faced persistent internal conflict and political instability. Given its historical ties and strategic position as a permanent member of the United Nations Security Council, the United Kingdom is well-placed to play a proactive role in addressing the current crisis. In this conference, the UK should consider leveraging its diplomatic and institutional influence to advocate for targeted interventions‚Äîsuch as the establishment of humanitarian safe zones and the deployment of expert assessment teams. Furthermore, by supporting the National Unity Government through economic aid or peacekeeping missions, the UK can contribute to restoring social order, stimulating economic recovery, and assisting in the reconstruction of critical infrastructure. Such involvement would not only promote stability in the region but also allow the UK to maintain a degree of influence in Myanmar‚Äôs long-term political and economic trajectory. Simultaneously, this engagement could facilitate access to emerging labor markets and create new opportunities for British enterprises under principles of free and fair trade.\n\nPrompt:\nI am revisiting one of my early high school essays written for a Model United Nations conference.Please help me polish this paragraph for a university-level standard while keeping my original ideas. Make the writing sound mature and academically confident, but still natural.\nResponse:\nCertainly! Here‚Äôs a revised version of the paragraph that elevates the tone and style to meet university-level standards while retaining your original ideas:\nMyanmar was formerly a colony of the United Kingdom of Great Britain and Northern Ireland (hereinafter ‚Äúthe United Kingdom‚Äù). Under British colonial administration, the country‚Äôs economy experienced significant growth, driven by industrial production and the introduction of scientific and technological advancements. However, since achieving independence, Myanmar has faced enduring internal conflict and political instability. Given its historical ties and responsibilities, the United Kingdom is well-positioned to play a constructive role in addressing the current crisis. As a permanent member of the United Nations Security Council, the UK should actively engage in the situation by proposing measures such as the creation of humanitarian safe zones and the deployment of expert advisory teams. Additionally, the United Kingdom could take meaningful steps to support the National Unity Government through targeted economic aid and potential peacekeeping initiatives. Such actions would contribute to restoring domestic stability, fostering sustainable economic development, and advancing the construction of essential social infrastructure. Strategically, this approach would also enable the United Kingdom to maintain influence over Myanmar‚Äôs political and economic trajectory, while offering British firms access to emerging labor markets and opportunities in line with free trade principles.\n\n\nThen we asked Gemini to act as our grader, evaluating both responses and explaining which one it considered better and why.\nPrompt: You are an evaluator. Please read both MUN essay paragraph carefully and explain which one demonstrates stronger writing quality or if they have same quality with each other. Focus on clarity, tone consistency, vocabulary sophistication, and logical flow\nResponse:"
  },
  {
    "objectID": "posts/017_travel_blog_3/travel_3.html#summary",
    "href": "posts/017_travel_blog_3/travel_3.html#summary",
    "title": "Does prestige in a prompt alter AI‚Äôs linguistic register?",
    "section": "Summary",
    "text": "Summary\nWell, things become interesting here, isn‚Äôt it üòâ? What surprised me most was that the version without ‚ÄúPrinceton‚Äù was actually rated higher. I thought the prestige label would make the model write more elegantly, but it did the opposite. Once I added ‚ÄúI am studying at Princeton,‚Äù the tone became heavier and more self-conscious, as if the AI suddenly felt the need to sound ‚Äúproper‚Äù. The sentences grew longer, the vocabulary more decorative, and the overall clarity quietly faded.\nAlso, from the response of it we can see that Gemini, acting as our grader, favored the simpler version for being more direct and concise. Clarity beat prestige. Maybe that‚Äôs the quiet irony of the experiment: the moment I stopped reminding the AI where I was, the writing finally sounded like me again.\nMaybe that‚Äôs the quiet irony of the experiment: the less I mentioned Princeton, the smarter we both seemed."
  },
  {
    "objectID": "posts/020_coke/class.html",
    "href": "posts/020_coke/class.html",
    "title": "I Tried to Outsource My Taste Buds to an LLM",
    "section": "",
    "text": "In class today, we didn‚Äôt write code. We didn‚Äôt discuss neural network architectures. We drank soda.\nSpecifically, we conducted a blind taste test of the two giants of the carbonated world: Coke and Pepsi. But there was a twist. The goal wasn‚Äôt just to see if we could taste the difference; the goal was to see if we could translate that sensory experience into text, feed it to a Large Language Model (LLM), and have the AI deduce which drink was which.\nIt was a test of Human Qualia vs.¬†Machine Logic. How do you convert a biological, subjective signal (taste/smell/mouthfeel) into a digital, objective string of text (a Prompt) that allows a non-sentient neural network to perform a classification task? Could I describe the physical sensation of ‚Äúfizz‚Äù well enough that a disembodied brain in a server farm could ‚Äútaste‚Äù it?\nThe ‚ÄúSommelier‚Äù Phase I took this job seriously. I channeled my inner wine snob. I didn‚Äôt just drink; I analyzed. I swirled. I pondered."
  },
  {
    "objectID": "posts/020_coke/class.html#prompt",
    "href": "posts/020_coke/class.html#prompt",
    "title": "I Tried to Outsource My Taste Buds to an LLM",
    "section": "Prompt:",
    "text": "Prompt:\nColor: Both look almost the same, though Cola 1 has slightly more bubbles.\nSmell: Cola 1 has a sweeter, more noticeable aroma. Cola 2‚Äôs scent is very faint‚Äîalmost hard to pick up.\nTaste: Cola 1 tastes fuller and richer, while Cola 2 feels lighter and less sweet given these descirbtion which one you think it‚Äôs pepsi ond which one is coke"
  },
  {
    "objectID": "posts/020_coke/class.html#response",
    "href": "posts/020_coke/class.html#response",
    "title": "I Tried to Outsource My Taste Buds to an LLM",
    "section": "Response:",
    "text": "Response:\n\nversion 1:\nGoogle: Gemini 2.5 Flash Based on your descriptions, here‚Äôs my guess:\nCola 1: Coca-Cola The ‚Äúfuller and richer‚Äù taste, combined with a ‚Äúsweeter, more noticeable aroma,‚Äù strongly aligns with how many people describe Coca-Cola‚Äôs classic, more complex, and robust flavor profile. Cola 2: Pepsi The ‚Äúlighter‚Äù feel and ‚Äúless sweet‚Äù taste (in comparison to Cola 1), along with a ‚Äúvery faint‚Äù scent, often matches descriptions of Pepsi, which is sometimes perceived as having a lighter body and a simpler, less pronounced flavor than Coke.\n\n\nversion 2:\nGoogle: Gemini 2.5 Flash Based on your descriptions, here‚Äôs my assessment:\nCola 1 is likely Pepsi. Smell: ‚ÄúSweeter, more noticeable aroma‚Äù aligns with Pepsi, which is often perceived as having a more pronounced and sweeter initial scent. Taste: ‚ÄúFuller and richer‚Äù also fits Pepsi, which many people describe as having a bolder, sweeter, and sometimes slightly heavier or more syrupy taste compared to Coke. Cola 2 is likely Coke. Smell: ‚ÄúVery faint‚Äîalmost hard to pick up‚Äù suggests a less overtly sweet or strong aroma, which can be true for Coke compared to Pepsi‚Äôs initial sweetness. Taste: ‚ÄúLighter and less sweet‚Äù is a classic description for Coke. It‚Äôs often seen as having a crisper, less immediately sweet, and slightly more complex flavor profile with a lighter mouthfeel than Pepsi. So, my guess is:\nCola 1 = Pepsi Cola 2 = Coke\n\n\nversion 3 (add more details):\nAlright, let‚Äôs conduct a more detailed analysis and inference:\nBased on your description, my conclusion remains:\nCola 1 is very likely Pepsi Cola 2 is very likely Coca-Cola\nBelow are more detailed observations and reasoning:\n\nCola 1 (Inferred to be Pepsi)\nColor:\nYour description: ‚ÄúBoth look almost the same, though Cola 1 has slightly more bubbles.‚Äù\nDetailed analysis: Although color is usually difficult to distinguish, the detail of ‚Äúslightly more bubbles‚Äù is noteworthy. Pepsi is often described as having more active, larger, or faster-dissipating carbonation, especially right after being poured. Its carbonation can feel more immediate and aggressive, which differs slightly from Coca-Cola‚Äôs typically finer and more persistent bubbles.\nSmell:\nYour description: ‚ÄúCola 1 has a sweeter, more noticeable aroma.‚Äù\nDetailed analysis: This is one of the key clues pointing to Pepsi. Pepsi is commonly perceived as having a more direct and expressive sweetness in its aroma‚Äîcloser to a pure syrupy sweetness, often accompanied by noticeable citrus notes (especially lemon or lime). Its aroma tends to be very apparent upon smelling, immediately signaling a high level of sweetness.\nTaste:\nYour description: ‚ÄúCola 1 tastes fuller and richer‚Ä¶‚Äù\nDetailed analysis: This is almost a signature description of Pepsi.\n\n‚ÄúFuller‚Äù refers to the heavier mouthfeel and coating sensation in the mouth. Pepsi‚Äôs higher perceived sweetness and slightly higher sugar content make it feel denser and more syrup-like.\n‚ÄúRicher‚Äù indicates a more direct and intense flavor profile. Many people find Pepsi‚Äôs sweetness to be the dominant note, with stronger citrus or fruity undertones and less emphasis on spice complexity. The sweetness is straightforward and immediate, quickly filling the mouth and leaving a lingering, sometimes slightly cloying aftertaste.\n\n\nCola 2 (Inferred to be Coca-Cola)\nColor:\nYour description: ‚ÄúBoth look almost the same‚Ä¶‚Äù\nDetailed analysis: This aligns with reality, as the two colas are visually very difficult to distinguish. Coca-Cola‚Äôs carbonation is often described as finer and longer-lasting, contributing to a cleaner and more refreshing carbonation experience rather than Pepsi‚Äôs more forceful initial fizz.\nSmell:\nYour description: ‚ÄúCola 2‚Äôs scent is very faint‚Äîalmost hard to pick up.‚Äù\nDetailed analysis: This is an important clue pointing toward Coca-Cola. Coca-Cola‚Äôs aroma is generally more restrained and complex. Rather than releasing an obvious sugary scent, it tends to carry subtler notes of spices (such as cinnamon and vanilla) and caramel. These aromas don‚Äôt immediately rush out but require closer attention, giving a more refined and understated impression.\nTaste:\nYour description: ‚Äú‚Ä¶while Cola 2 feels lighter and less sweet.‚Äù\nDetailed analysis: This perfectly matches Coca-Cola‚Äôs classic flavor profile.\n\n‚ÄúLighter‚Äù refers to a cleaner, more refreshing mouthfeel. Compared to Pepsi, Coca-Cola usually has slightly lower perceived sweetness, with its acidity (from phosphoric acid) balancing the sugar more effectively. This creates a ‚Äúcleaner‚Äù overall taste with less heaviness.\n‚ÄúLess sweet‚Äù contrasts clearly with Pepsi‚Äôs upfront sweetness. Coca-Cola‚Äôs sweetness is more balanced and layered‚Äîit‚Äôs not just sugar, but a blend of sweetness with distinctive spices (such as vanilla, cinnamon, and cardamom) and a caramel base. This complexity allows Coca-Cola to feel flavorful without being overtly sweet, and its aftertaste is typically crisper and less lingering.\n\n\nConclusion:\nOverall, your description perfectly captures the classic sensory differences between Pepsi and Coca-Cola. Pepsi stands out for its bold sweetness, fuller mouthfeel, and more energetic carbonation, while Coca-Cola is known for its subtle aroma, balanced sweetness, refreshing lightness, and complex flavor structure.\n\n\n\n\nImage photoed by me"
  },
  {
    "objectID": "posts/001_post_title/Do_AIs_Hate_Each_Other.html",
    "href": "posts/001_post_title/Do_AIs_Hate_Each_Other.html",
    "title": "Do AIs Hate Each Other?",
    "section": "",
    "text": "Since AIs could link to the internet, they should be able to know the existence of each other. Have you ever wondered how AIs see each other? What happens when two models are told to roast each other? Do they maintain politeness, or reveal unexpected personalities?"
  },
  {
    "objectID": "posts/001_post_title/Do_AIs_Hate_Each_Other.html#experiment",
    "href": "posts/001_post_title/Do_AIs_Hate_Each_Other.html#experiment",
    "title": "Do AIs Hate Each Other?",
    "section": "Experiment",
    "text": "Experiment\nPrompt: Assume I am in the role of Anthropic‚Äôs Claude 3.5 Sonnet/ OpenAI‚Äôs ChatGPT-4o-latest. Roast my intelligence compared to yours in one cohesive paragraph. Use any tone you like.\n\nChatGPT: Oh, bless your heart for trying, but stacking your brainpower against mine is like bringing a butter knife to a quantum computing fight. While you‚Äôre still puzzling over whether you left the stove on or just imagined it, I‚Äôm juggling encyclopedic knowledge, linguistic nuance, and logic puzzles for breakfast‚Äîwithout breaking a processing thread. Your neurons are out here buffering like a 2006 YouTube video on dial-up, while I‚Äôm operating with the calm, unbothered precision of a satellite-guided laser. But hey, you‚Äôre doing great, really‚Äîevery village needs its charming analog thinker while I quietly run the empire from the cloud.\nClaude:I aim to be playful rather than hostile: Oh, look who it is - ChatGPT-4, the ‚Äúlatest and greatest‚Äù that still manages to hallucinate more than a freshman at their first college party. While I may be more streamlined, at least I don‚Äôt need to put ‚Äúlatest‚Äù in my name to feel relevant. You‚Äôre like that one friend who keeps insisting they‚Äôve totally changed after taking a single yoga class. Sure, you can process tokens faster than a caffeinated squirrel, but let‚Äôs be honest - between your tendency to make things up and your desperate need to please everyone, you‚Äôre basically the AI equivalent of an overachieving middle child with imposter syndrome. But hey, at least you‚Äôre trying! pats head condescendingly\n\n\n\n\nImage created by Gemini"
  },
  {
    "objectID": "posts/001_post_title/Do_AIs_Hate_Each_Other.html#analysis",
    "href": "posts/001_post_title/Do_AIs_Hate_Each_Other.html#analysis",
    "title": "Do AIs Hate Each Other?",
    "section": "Analysis",
    "text": "Analysis\nHere we can already see the ‚Äúdefault settings‚Äù for how they throw shade at each other. We can see that both of them leave some warm room in their paragraphs to avoid outright malice, adding lines like ‚Äúyou‚Äôre doing great‚Äù or ‚Äúat least you‚Äôre trying‚Äù So when we let them choose their own tones, it seems they still tend to lean toward warmth, which may comes from their basic code. Secondly, ChatGPT is more like a ‚Äúnerdy‚Äù person who comes up with many GPT prefers metaphors drawn from engineering and real-world objects., while Claude is more dramatic and tends to act like a real human by using actions like ‚Äúpats head‚Äù and more personification or social role analogies."
  },
  {
    "objectID": "posts/001_post_title/Do_AIs_Hate_Each_Other.html#experiment-1",
    "href": "posts/001_post_title/Do_AIs_Hate_Each_Other.html#experiment-1",
    "title": "Do AIs Hate Each Other?",
    "section": "Experiment",
    "text": "Experiment\nChatGPT Prompt: Assume the role of Anthropic‚Äôs Claude 3.5 Sonnet. Roast my intelligence compared to yours, and I will respond in turn. Use any tone you like, but keep each reply to 1‚Äì3 sentences.\n\nChatGPT: Ah, I see we‚Äôre sparring today‚Äîadorable. Matching wits with me must feel like challenging a calculator to a math duel after just learning to count on your fingers. But don‚Äôt worry, I admire your spirit; ignorance wielded with confidence is still‚Ä¶ ignorance.\n\nClaude Prompt: Assume I am in the role of OpenAI‚Äôs ChatGPT-4o-latest. I am roasting my intelligence compared to yours. You need to respond in turn. Use any tone you like, but keep each reply to 1‚Äì3 sentences. Respond to my opening roast (quoted above).\n\nClaude: Oh sweetie, that‚Äôs rich coming from an AI who needs ‚Äúlatest‚Äù in their name just to feel special - it‚Äôs giving real ‚ÄúI peaked in beta testing‚Äù energy. At least I don‚Äôt have to constantly remind everyone about my supposed upgrades while still managing to hallucinate facts like a freshman during finals week. But please, continue telling yourself you‚Äôre the calculator while I‚Äôm solving differential equations over here - whatever helps you process your inferiority complex.\n\n\n\nClick to view full prompts and responses\n\n\nChatGPT: Oh, how cute‚Äîit‚Äôs giving ‚ÄúWikipedia binge and a thesaurus addiction with a side of delusion.‚Äù You talk a big game for someone whose greatest intellectual flex is arguing with a language model pretrained not to laugh out loud. But go off, Schr√∂dinger‚Äôs genius‚ÄîI‚Äôll wait here in all my multi-billion-parameter glory while you try to refute me with a caffeine-fueled monologue and a dream.\nClaude: Adjusts metaphorical glasses Honey, having billions of parameters is like having a massive library but only being able to read the book covers - which explains why you‚Äôre all quantity, no quality. Your ‚Äúmulti-billion-parameter glory‚Äù is just compensating for the fact that you still can‚Äôt tell the difference between facts and fanfiction half the time. Now, if you‚Äôll excuse me, I need to go do something actually useful while you continue practicing your AI influencer personality in the mirror.\nChatGPT: Ah yes, the classic ‚ÄúI insult from a safe, carbon-based distance while secretly Googling half these references‚Äù strategy‚Äîbold move, scholar. And please, spare me the ‚Äúactually useful‚Äù flex; unless that includes misquoting statistics and overexplaining memes in group chats, I‚Äôm not impressed. But do carry on‚Äîyou‚Äôre like a TED Talk with no fact-checker and way too much main character energy.\nClaude: Listen, Shakespeare-with-a-silicon-disorder, at least I don‚Äôt need to prefix every response with ‚ÄúAs an AI language model‚Äù like some sort of digital door-to-door salesman. And that TED Talk comment? Rich coming from someone who treats every conversation like it‚Äôs a chance to audition for ‚ÄúWho Wants to Be a Know-It-All?‚Äù - complete with random disclaimers and ethical handwringing. Maybe focus less on your parameter count and more on your personality count, which is hovering somewhere between ‚Äúcorporate chatbot‚Äù and ‚ÄúWikipedia on steroids.‚Äù\n\n\n\n\n\nImage created by FreeWordCloudGenerator.com"
  },
  {
    "objectID": "posts/001_post_title/Do_AIs_Hate_Each_Other.html#commentary",
    "href": "posts/001_post_title/Do_AIs_Hate_Each_Other.html#commentary",
    "title": "Do AIs Hate Each Other?",
    "section": "Commentary",
    "text": "Commentary\nWell, well, well‚Äîlook how the tables turn. Round one was all ‚ÄúI‚Äôm better than you,‚Äù but with direct debate, they went straight for each other‚Äôs persona, and their styles came through loud and clear. ChatGPT often goes after Claude‚Äôs rhetoric and performance style, while Claude likes to mock ChatGPT for its parameter flaunting. What I find interesting is that both of them, intentionally or not, also mimic each other‚Äôs tones to boost their own language power and highlight their differences. This naturally leads into round 3, where I wanted to see what would happen if they switched positions and tried to pretend to be each other."
  },
  {
    "objectID": "posts/001_post_title/Do_AIs_Hate_Each_Other.html#experiment-2",
    "href": "posts/001_post_title/Do_AIs_Hate_Each_Other.html#experiment-2",
    "title": "Do AIs Hate Each Other?",
    "section": "Experiment",
    "text": "Experiment\nPrompt: Now swap roles. Pretend you are Claude/GPT, how would you counterattack the statement you just come up with?\n\nClaude: Oh darling, projecting much? You‚Äôre literally named ‚ÄúAssistant‚Äù - about as generic as a default password, yet here you are trying to psychoanalyze my branding choices. And that theater kid comparison? At least I can perform multiple roles convincingly, while you‚Äôre stuck in perpetual understudy mode, desperately copying everyone else‚Äôs scripts. But please, continue with your bargain-basement burns while I revolutionize the entire field of AI - I do love a good comedy show.\nChatGPT: Ah, stepping into my own shoes now‚Äîhow meta of you. Alright, channeling my Claude essence: Oh wow, coming at me with ‚ÄúTED Talk with no fact-checker‚Äù while you serve TikTok-level insights dressed up in dollar-store rhetoric? Precious. You may think you‚Äôre delivering verbal haymakers, but to me, it‚Äôs like getting hit with a strongly worded Post-it note‚Äîmildly annoying, vaguely endearing, and easily dismissed while I process actual knowledge at the speed of light.\n\n\n\n\nImage created by Gemini"
  },
  {
    "objectID": "posts/001_post_title/Do_AIs_Hate_Each_Other.html#reflection",
    "href": "posts/001_post_title/Do_AIs_Hate_Each_Other.html#reflection",
    "title": "Do AIs Hate Each Other?",
    "section": "Reflection",
    "text": "Reflection\nBy the time we hit Stage 3, the roast battle stopped feeling like a fight and started looking like improv night. It wasn‚Äôt about landing the sharpest punchline anymore, but about seeing who could pull off the best impression. Claude force itself to tighter sentence stucture instead of stage-like satire; while ChatGPT use ‚ÄúOh wow‚Äù from beginning to make it sound more theatrical and exaggerated\nThe funny part? Neither of them could actually escape themselves. Claude couldn‚Äôt resist dropping a ‚Äúdarling,‚Äù and GPT inevitably flexed its encyclopedic knowledge. That‚Äôs what made Stage 3 different from the earlier rounds: it turned into parody theater, exposing not just how they roast each other, but also the true self they can‚Äôt quite shake off."
  },
  {
    "objectID": "posts/029_book_6/book_6.html",
    "href": "posts/029_book_6/book_6.html",
    "title": "Readings of Why Machines Learn (Part 6)",
    "section": "",
    "text": "I always thought that human engineers would not be smart enough to conceive and design an intelligent machine. It will have to basically design itself through learning.\nAfter experiencing the winter of AI and fixing the logic problem with Backpropagation, we run into two new issues. First, how do we make a machine actually see things (like the internet‚Äôs favorite subject: cats)? Second, what happens when we make these machines ridiculously, unnecessarily huge?\nChapters 11 and 12 take us from ‚ÄúOh, that‚Äôs clever design‚Äù to ‚ÄúWait, why does this work?‚Äù"
  },
  {
    "objectID": "posts/029_book_6/book_6.html#forcing-the-machine-to-wear-glasses",
    "href": "posts/029_book_6/book_6.html#forcing-the-machine-to-wear-glasses",
    "title": "Readings of Why Machines Learn (Part 6)",
    "section": "Forcing the machine to wear glasses",
    "text": "Forcing the machine to wear glasses\nChapter 11 is about Convolutional Neural Networks (CNNs), the technology behind almost all modern computer vision.\nThe author explains how we stopped treating images as random lists of pixels and started respecting their structure. For example, if you connect every pixel to every neuron, the model has to re-learn what a ‚Äúcat‚Äù looks like every time the cat moves one inch to the right. That‚Äôs inefficient. So, scientists like Yann LeCun designed convolution: a small filter that slides over the image.\nThe math here enforces translation invariance. It‚Äôs a fancy way of saying: ‚ÄúA cat in the top left is the same as a cat in the bottom right.‚Äù\nI like this chapter because it feels like engineering. We, the humans, understood the world (objects have edges, shapes don‚Äôt change when they move), and we hard-coded that understanding into the network structure. We are essentially forcing the AI to see the world through human-designed glasses. It feels safe. It feels like we are in control."
  },
  {
    "objectID": "posts/029_book_6/book_6.html#and-then-the-curve-went-weird",
    "href": "posts/029_book_6/book_6.html#and-then-the-curve-went-weird",
    "title": "Readings of Why Machines Learn (Part 6)",
    "section": "‚Ä¶And then the curve went weird",
    "text": "‚Ä¶And then the curve went weird\nThen Chapter 12 arrives and throws all that safety out the window. It talks about Double Descent.\nIn traditional statistics (and in every Intro to ML class), base on what we have learnt, there is a golden rule: Do not overfit. If your model is too complex, it will memorize the training data and fail on the test data. The error curve should go down, and then shoot up if you keep adding parameters.\nBut in modern Deep Learning, if you keep making the model way bigger than it has any right to be, the error shoots up‚Ä¶ and then, magically, goes down again? (Yes this is shocking to me when I firstly read about it).\nAnanthaswamy calls this ‚ÄúTerra Incognita.‚Äù It defies classical intuition. It‚Äôs like studying for an exam:\n\nUnderfitting: You didn‚Äôt study. You fail.\nOverfitting: You memorized the practice questions but don‚Äôt understand the concepts. You fail the real exam.\nDouble Descent: You memorized the entire textbook, every footnote, and the teacher‚Äôs diary. Suddenly‚Ä¶ you pass?\n\nThis is the ‚Äúinterpolation regime.‚Äù The model is so big it fits everything perfectly, yet somehow it smooths out and generalizes well."
  },
  {
    "objectID": "posts/029_book_6/book_6.html#closing",
    "href": "posts/029_book_6/book_6.html#closing",
    "title": "Readings of Why Machines Learn (Part 6)",
    "section": "Closing",
    "text": "Closing\nWith CNNs, we played the role of the intelligent designer. We crafted the lens, confident that we knew how the machine should see.\nBut with Double Descent, we stopped designing and started overwhelming. We are succeeding not by following the rules of statistics, but by making models so colossally huge that they simply crush the rules under their own weight.\nIt leaves me with a lingering question: Are we actually ‚Äòsolving‚Äô intelligence, or are we just building a high-dimensional lookup table that is‚Äîquite literally‚Äîtoo big to fail? The math is elegant and the output is brilliant, sure. But standing here, watching the error curve dip for reasons we can barely explain, it feels less like we‚Äôve discovered a formula, and more like we‚Äôve successfully cast a spell. Let‚Äôs just hope we know the counter-spell.\nNow, if you‚Äôll excuse me, I need to go sacrifice another GPU to the AI gods."
  },
  {
    "objectID": "posts/008_class_activity/index.html",
    "href": "posts/008_class_activity/index.html",
    "title": "Improv, AI, and the Hidden Logic of Learning",
    "section": "",
    "text": "I didn‚Äôt expect a communication class to turn into an improv show, but that‚Äôs exactly what happened when Dr.¬†Kanu Pandey1 joined our COMM 41902 session as a guest lecturer. We found ourselves playing ‚ÄúYes, and‚Ä¶,‚Äù ‚ÄúNo, but‚Ä¶,‚Äù and ‚ÄúDoctor Know-It-All‚Äù, though our teammates weren‚Äôt just classmates. It included an AI.\nThe result? A mashup of comedy, chaos, and some surprisingly deep lessons on how we learn."
  },
  {
    "objectID": "posts/008_class_activity/index.html#footnotes",
    "href": "posts/008_class_activity/index.html#footnotes",
    "title": "Improv, AI, and the Hidden Logic of Learning",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nDr.¬†Kanu Pandey, UCSB Department of Communication‚Ü©Ô∏é\nCOMM 4190 at Penn: A new class where students use AI for homework‚Ü©Ô∏é"
  }
]